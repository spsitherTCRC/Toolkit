{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome","text":"<p>OpenPecha is an  etext and  annotations store made available on GitHub and through a set of APIs. </p> <p>The project\u2019s primary aim is to facilitate the collection, proofreading, and enrichment of etexts by leveraging language technology and collaboration.</p> <p>New to OpenPecha? Here are a few places to get started using our data and tools:</p> <ul> <li> <p> Download a featured dataset</p> <p>Get the latest Pecha datasets to train Tibetan-language AI models.</p> <p> Featured datasets</p> </li> <li> <p> OCR scanned BDRC books</p> <p>Use the OCR Pipeline to OCR scans in the BDRC collection.</p> <p> OCR books  </p> </li> <li> <p> Get Pecha Toolkit</p> <p>Install <code>Pecha Toolkit</code> with <code>pip</code> and get up and running in minutes.</p> <p> Pecha toolkit</p> </li> <li> <p> Get the latest news</p> <p>Read our blog to learn the latest from OpenPecha and the Tibetan AI space.</p> <p> OpenPecha blog</p> </li> </ul>"},{"location":"benchmarks/","title":"Benchmarks","text":""},{"location":"api/getting-started/","title":"Getting started with the web API","text":""},{"location":"api/reference/","title":"Web API reference","text":""},{"location":"blog/","title":"OpenPecha blog","text":""},{"location":"blog/2022/11/26/coming-soon/","title":"Coming soon!","text":"<p>Stay tuned for our new blog.</p>"},{"location":"community/connect/","title":"Connect","text":"<p>Have questions or want to reach out to us?</p> <ul> <li>Email us at openpecha [at] gmail [dot] com.</li> <li>Join the OpenPecha Discord and ask us there.</li> </ul>"},{"location":"community/partners/","title":"Partners","text":"<p>Indrajala powers Buddhist technology through the gift of data.</p> <p></p> <p>The Buddhist Digital Resource Center is a nonprofit organization dedicated to seeking out, preserving, documenting, and disseminating Buddhist literature.</p> <p></p> <p>Monlam AI develops artificial intelligence in service of Tibetan language and culture.</p> <p></p> <p>Esukhia is a network of schools, teachers, and researchers who specialize in resources dedicated to the Tibetan languages and their textual traditions.</p> <p></p> <p>The Kumarajiva Project aims to translate into Chinese all the texts in the Tibetan Buddhist canon that are not currently available in the Chinese canon.</p> <p></p> <p>Lotus King Trust supports the propagation and practice of Buddhadharma and the meaningful development of remote Himalayan communities.</p> <p></p> <p>pecha.jobs provides data sourcing and processing services for individuals and organizations working to preserve and promote the Tibetan language, culture, and religion.</p>"},{"location":"community/stay-informed/","title":"Stay informed","text":"<p>Want to keep up with latest developments at OpenPecha?</p> <ul> <li>Check out our Action Roadmap and FY23 Workplan on GitHub.</li> <li>Read our blog.</li> <li>Join the OpenPecha Discord.</li> </ul>"},{"location":"data/awesome-tibetan-canon/","title":"Awesome Tibetan Canon","text":"<p>Datasets and resources for the Tibetan canon </p>"},{"location":"data/awesome-tibetan-canon/#contents","title":"Contents","text":"<ul> <li>Etexts</li> <li>Images</li> <li>Community</li> <li>Resources</li> </ul>"},{"location":"data/awesome-tibetan-canon/#etexts","title":"Etexts","text":""},{"location":"data/awesome-tibetan-canon/#kangyur","title":"Kangyur","text":"<ul> <li>ACIP Derge Kangyur - Lhasa input proofread against Derge, vol. xx missing</li> <li>SOAS-THL Derge Kangyur -  several input compared and proofread against LOC Derge scans, missing end of vol. 100, vol. 103 </li> <li>Esukhia Derge Kangyur - SOAS-THL further proofread with scripts, annotated, missing vol. 103</li> <li>Adarsha Lithang Kangyur - Lithang input proofread against Adarsha scans, annotated</li> </ul>"},{"location":"data/awesome-tibetan-canon/#tengyur","title":"Tengyur","text":"<ul> <li>ACIP Derge Tengyur - Derge input </li> <li>Esukhia Derge Tengyur - Namsel OCR and ACIP etexts compared and proofread against Esukhia scans, further proofread with scripts, annotated</li> </ul>"},{"location":"data/awesome-tibetan-canon/#images","title":"Images","text":""},{"location":"data/awesome-tibetan-canon/#websites","title":"Websites","text":""},{"location":"data/awesome-tibetan-canon/#kangyur_1","title":"Kangyur","text":"<ul> <li>rKTs - wow!</li> <li>BDRC Kangyurs (legacy) - Scanned Kangyurs</li> <li>BDRC Kangyurs (BUDA) - Scanned Kangyurs</li> <li>EAP collection 570, EAP collection 039, EAP collection 310 - scanned collection including several canons</li> </ul>"},{"location":"data/awesome-tibetan-canon/#tengyur_1","title":"Tengyur","text":"<ul> <li>BDRC Tengyurs (legacy) - Scanned Tengyurs</li> <li>BDRC Tengyurs (BUDA) - Scanned Tengyurs</li> </ul>"},{"location":"data/awesome-tibetan-canon/#community","title":"Community","text":"<ul> <li>TibetTech Slack (<code>#canon</code> channel)</li> </ul>"},{"location":"data/awesome-tibetan-canon/#resources","title":"Resources","text":""},{"location":"data/awesome-tibetan-nlp/","title":"awesome-tibetan-nlp","text":"<p>Tibetan NLP projects and resources</p>"},{"location":"data/awesome-tibetan-nlp/#contents","title":"Contents","text":"<ul> <li>Datasets</li> <li>OCR</li> <li>Speech recognition</li> <li>Machine Translation</li> <li>Cleanup</li> <li>Word segmentation</li> <li>Sentence boundary disambiguation</li> <li>Lemmatization</li> <li>Word sense disambiguation</li> <li>POS tagging</li> <li>Dependency parsing</li> <li>Coreference_resolution</li> <li>Spellchecking</li> <li>NER - Named Entity Recognition</li> <li>IR - Information Retrival</li> <li>Text summarization</li> <li>Summarization</li> <li>Text similarity</li> <li>Community</li> <li>Resources</li> </ul>"},{"location":"data/awesome-tibetan-nlp/#datasets","title":"Datasets","text":""},{"location":"data/awesome-tibetan-nlp/#corpora","title":"Corpora","text":"<ul> <li>Tibetan Corpora on Zenodo</li> <li>Tibetan Corpus on Sketchengine</li> </ul>"},{"location":"data/awesome-tibetan-nlp/#models","title":"Models","text":"<ul> <li>Fastext Vector</li> </ul>"},{"location":"data/awesome-tibetan-nlp/#ocr","title":"OCR","text":"<ul> <li>Namsel: An Optical Character Recognition System for Tibetan Text</li> </ul>"},{"location":"data/awesome-tibetan-nlp/#speech-recognition","title":"Speech recognition","text":"<ul> <li>An Improved Tibetan Lhasa Speech Recognition Method Based on Deep Neural Network, 2017</li> <li>Tibetan-Mandarin bilingual speech recognition based on end-to-end framework, 2017</li> <li>Deep Feature Learning for Tibetan Speech Recognition using Sparse Auto-encoder, 2015</li> </ul>"},{"location":"data/awesome-tibetan-nlp/#machine-translation","title":"Machine Translation","text":"<ul> <li>Tibetan-Chinese Neural Machine Translation based on Syllable Segmentation (Compared Syllable Segmentation with Word Segmentation), 2018</li> </ul>"},{"location":"data/awesome-tibetan-nlp/#cleanup","title":"Cleanup","text":""},{"location":"data/awesome-tibetan-nlp/#word-segmentation","title":"Word segmentation","text":"<ul> <li>An Algorithm Rapidly Segmenting Chinese Sentences into Individual Words, 2019</li> <li>Research and Implementation of Tibetan Word Segmentation Based on Syllable Methods, 2018</li> <li>Segmenting and POS tagging Classical Tibetan using a Memory-Based Tagger, 2017</li> <li>Towards describing Tibetan syntax: From word segmentation to rewrite rules through a semi-automated workflow, 2016</li> </ul>"},{"location":"data/awesome-tibetan-nlp/#sentence-boundary-disambiguation","title":"Sentence boundary disambiguation","text":""},{"location":"data/awesome-tibetan-nlp/#lemmatization","title":"Lemmatization","text":"<ul> <li>The methods of lemmatization of bound case markers in modern Tibetan</li> </ul>"},{"location":"data/awesome-tibetan-nlp/#word-sense-disambiguation","title":"Word sense disambiguation","text":""},{"location":"data/awesome-tibetan-nlp/#pos-tagging","title":"POS tagging","text":"<ul> <li>Segmenting and POS tagging Classical Tibetan using a Memory-Based Tagger, 2017</li> </ul>"},{"location":"data/awesome-tibetan-nlp/#dependency-parsing","title":"Dependency parsing","text":""},{"location":"data/awesome-tibetan-nlp/#coreference_resolution","title":"Coreference_resolution","text":""},{"location":"data/awesome-tibetan-nlp/#spellchecking","title":"Spellchecking","text":""},{"location":"data/awesome-tibetan-nlp/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"data/awesome-tibetan-nlp/#information-retrival","title":"Information Retrival","text":""},{"location":"data/awesome-tibetan-nlp/#text-summarization","title":"Text summarization","text":""},{"location":"data/awesome-tibetan-nlp/#summarization","title":"Summarization","text":""},{"location":"data/awesome-tibetan-nlp/#text-similarity","title":"Text similarity","text":""},{"location":"data/awesome-tibetan-nlp/#community","title":"Community","text":"<ul> <li>TibetTech Slack</li> </ul>"},{"location":"data/awesome-tibetan-nlp/#resources","title":"Resources","text":"<ul> <li>Introduction (to special issue on Tibetan Natural Language Processing)</li> <li>Practical Applications for Corpora: The Role of Research-based Linguistics in Literacy &amp; Education for the Tibetan Language</li> </ul>"},{"location":"data/bdrc-and-google-books/","title":"Building BDRC\u2019s Tibetan E-text Corpus","text":""},{"location":"data/bdrc-and-google-books/#a-bdrc-google-books-collaborative-project","title":"A BDRC / Google Books Collaborative Project","text":"<p>Support from various funders has enabled BDRC to enter new frontiers in the areas of cultural preservation and digital technology. As the leading digital Buddhist library for more than twenty years, BDRC has a uniquely significant collection of Tibetan texts and serves as a repository for endangered Tibetan cultural heritage materials. </p> <p>In 2019, Google invited BDRC to its campus in Sunnyvale to deliver a talk, meet its engineers, and discuss Tibetan computing, digital books, and language tools. From this initial meeting, BDRC and Google began working together to create e-texts from BDRC scans acquired in Tibetan communities. Supported by a $10,000 credit from Google, BDRC began experimenting using Google\u2019s cutting-edge OCR (optical character recognition) engine to generate machine-readable texts from digital images of texts. Now, a renewed partnership with Google Books promises to transform BDRC\u2019s scanned image library into a dataset of approximately 4 billion words, about the size of the English version of Wikipedia. Such a vast dataset will make it possible to develop crucial Tibetan-language digital tools that have long been taken for granted in other languages. For the first time, Tibetan search engines, reading and literacy tools, writing and publishing industries, and translation and lexicography projects will have access to the resources they need to bring the Tibetan language into the modern age.</p>"},{"location":"data/bdrc-and-google-books/#applications-of-the-dataset","title":"Applications of the Dataset","text":"<p>Search Engines</p> <p>The dataset generated by the partnership with Google\u2014combined with the work that the BDRC team did in 2020 on lexical resources\u2014will allow corporations to build functional Tibetan search engines. Surprisingly, current search engines fail to retrieve useful results when used to search for Tibetan words. This is because Tibetan is a so-called continuous script language where words are not separated by spaces. Thus, search engines don\u2019t know where words start or end and can\u2019t separate relevant and irrelevant results. **Not being able to rely on Google is difficult to imagine, but this is the situation for average Tibetans. They don\u2019t have easy access to massive amounts of information in their native language and many feel backwards because of this.</p> <p>With proper search engines, Tibetans will have information at their fingertips and will be able to learn new skills, gain confidence, progress in their careers, understand the world better, not have to rely on Chinese and English as much, and become part of the 21st century. </p> <p>Reading &amp; Literacy</p> <p>The dataset will also help improve literacy among Tibetan speakers by laying the foundation for Tibetan reading assessment software and digital reading aides.</p> <p>In major languages, there is reading material for every age and reading level. In addition, companies like Lexile and Scholastic use AI to assess the reading level of vast amounts of content, so that parents and teachers can choose the right book or text for children by age and reading level. Because Tibetan doesn\u2019t have this, Tibetan children are often given books far above their reading level. This makes learning to read harder and less enjoyable. In the not-too-distant future, Tibetan readers will also benefit from this.</p> <p>The tools available to speakers of major languages help readers consume texts above their level. For example, e-readers have embedded dictionaries to help look up words, text-to-speech engines to read along with, and other reading aides. These tools build confidence. Most Tibetans are native Tibetan speakers, yet they prefer to read in Chinese or English as there are tools to support them. This should change when these technologies are developed for the Tibetan language.</p> <p>Writing &amp; Publishing</p> <p>The dataset will allow corporations and startups to create writing tools like spellcheckers, grammar checkers, and style checkers. Without these, most of us wouldn\u2019t feel confident writing. The average Tibetan, who doesn\u2019t have access to these tools in Tibetan, also isn\u2019t confident writing in their own language. The result is that most Tibetans don\u2019t write, and even if they do, they don\u2019t write well. In fact, in Tibetan communities in India, there is a common idea that class 12 students can\u2019t write a job application in Tibetan or a letter to their mother. Good writing technology will change that.</p> <p>Having machine readable versions of the BDRC collection will save editors and publishers who wish to publish them a lot of time preparing text. They will also be able to compare different editions of books and create critical editions.</p> <p>The dataset will also form the basis of analytical tools to help publishers, online book distributors, and authors use analytics to better sell books. Other tools will help summarize books, classify books by topic, and provide search on publisher sites, offer book recommendations based on readers\u2019 behavior, and conduct sentiment analysis on reader feedback.</p> <p>Translation &amp; Lexicography</p> <p>This project\u2014along with tools that can now be built on top of it\u2014will allow for the creation of Tibetan language CAT (computer assisted translation) tools. This in turn will usher in the rapid translation of foriegn language texts into Tibetan and vastly increase the amount of content available in Tibetan.</p> <p>In the past 20 years, translators started using CAT tools. This improved translation efficiency by anywhere from 40-80 percent. Current state-of-the-art, tailored adaptive suggestions powered by Neural Machine Translation (NMT) is 3-5 times faster than previous technology. These tools are made possible by AI models trained on combinations of bi-lingual and single language datasets. BDRC\u2019s etext dataset will go a long way to help create these tools in Tibetan.</p> <p>In terms of lexicography, good dictionaries are at the heart of education and literacy. Every Tibetan student will tell you that looking up a word starts a long loop of having to look up other words to understand the definition etc. This is an issue English speakers rarely encounter since technology has long been used to pick the useful example sentences and to control the vocabulary used to define words. The Tibetan corpus built with this project will level the playing field for dictionary builders by making the best lexicographical tools available for the Tibetan language. For example, the corpus will be made available on Sketchegine, the tool that Oxford, Cambridge, and Collins dictionaries use.</p> <p>How This Affects Technology Startups and Freelancers</p> <p>Today and tomorrow\u2019s technology leverages AI to turn data and information into commodities. The bulk of intelligent or smart products and service automation rely on language technology. Predictions for the coming 10 years of AI innovation<sup>1</sup> forecast a symbiosis between big tech on one side and small startups and specialized freelancers on the other. Big companies train powerful yet blunt foundation models on massive amounts of data, which they then rent to startups and freelancers who fine-tune them with custom data for specialized purposes.</p> <p>BDRC\u2019s corpus will be usable at both levels. Since it is open, big companies will be able to include it when training foundation models, and freelancers will be able to curate and use specific subsets of the data. </p> <p>Both BDRC\u2019s dataset and models trained on it will open the doors to countless possibilities for Tibetan freelancers. Smart technology, innovative publication formats, advertisements, recommendations, and the like will become accessible to technology startups. </p> <p>As AI requires human expertise, a thriving data-driven technology economy will in turn create employment for literate Tibetans. For example, graduate students, who wouldn\u2019t otherwise have many job opportunities, can work as translators and annotators to create and fine-tune training data. In fact, in the past few years, a few training data creation projects (most likely for Bing) have employed many university graduates to create translation training data.</p> <p>BDRC\u2019s new dataset will greatly contribute to this budding industry.</p> <p>A Case Study</p> <p>The first case of big tech and specialized freelancer symbiosis is Bing Translator. Bing\u2019s foundation model for translation from Tibetan was trained on publicly available data and freelancers are welcome to fine-tune it with their own dictionaries and data and then sell the resulting translation engine for profit. Bing is decent when translating everyday language, but clearly wasn\u2019t trained on native literature. Judging from current translation results, it seems that the main literary dataset it was trained on was the Bible in Tibetan. </p> <p></p> <p>(Bing\u2019s current effort at translating the title \u201cA Guide to the Bodhisattva's Way of Life\u201d.)</p> <p>BDRC\u2019s e-text corpus combined with other bilingual corpus projects will provide Bing Translate with the data needed to significantly improve its quality. Tech startups and freelance translators will be able to use their own data to fine-tune Bing translation engine.</p> <p>In conclusion, in the same way that the impact of BDRC scans had a huge impact on cultural preservation, education, and publishing, the BDRC\u2019s e-text corpus will have an enormous impact on innovative technology and the Tibetan language industry of tomorrow.</p>"},{"location":"data/bdrc-and-google-books/#notes","title":"Notes","text":"<ol> <li> <p>Andrew Ng predicts the next 10 years in AI | VentureBeat \u21a9</p> </li> </ol>"},{"location":"data/core-concepts/","title":"Core concepts","text":""},{"location":"data/creating-clean-digital-pedurma/","title":"Creating clean digital pedurma","text":"<p>Steps that we have gone through preparing the opfs are as follow:</p> <ul> <li> <p>we have ocred the pedurma and format it in opf</p> </li> <li> <p>we parse the bdrc outline to get image number range of the texts, pg number range of texts, title, durchen page and image range</p> </li> <li> <p>we parsed title and text Id from rkts outline </p> </li> <li> <p>using the image number info of outlines and pagination layers, we tried to reconstruct the index of pedurma</p> </li> <li> <p>using the same info we annotated the durchen starting point and ending point</p> </li> <li> <p>after that we have serialize the annotations in html(human friendly markup language) </p> </li> <li> <p>given them to our freelancer to manually check the actual table of content</p> </li> <li> <p>reformat the updated hfmls to opf</p> </li> </ul> <p>** Regarding the derge Google pedurma, we have transfer all the base text to Google ocred opf base and updated all the layers. But we have left the volumes which are not in derge as it as in the opf</p>"},{"location":"data/data-samples/","title":"Librarian (metadata about etexts, unstructured content)","text":"<ul> <li>BDRC graphs - bibliographical information about Buddhist literature</li> <li>OP Works catalog - work catalog</li> <li>[OPF catalog](https://github.com/OpenPecha-Data/catalog/blob/master/data/catalog.csv - contains all instances of I#, D# and O#</li> <li>A# Alignment catalog</li> <li>W# Work - information about a work</li> <li>OPF Pecha</li> <li>C# Collection </li> </ul>"},{"location":"data/data-samples/#scholar-semi-structured-etext-content","title":"Scholar (semi-structured etext content)","text":"<ul> <li>I# Initial Pecha</li> <li>D# Diplomatic Edition</li> <li>Open Edition</li> <li>Alignment</li> <li>A# Alignment</li> </ul>"},{"location":"data/data-samples/#professional-structured-knowledge","title":"Professional (structured knowledge)","text":""},{"location":"data/edition-types/","title":"Edition types","text":"<p>We can define 5 main types of edition:</p>"},{"location":"data/edition-types/#facsimile-edition-scans","title":"Facsimile edition (scans)","text":"<p>A reproduction (now usually photographic) with commentary.</p>"},{"location":"data/edition-types/#diplomatic-edition-ocr-or-diplomatic-input-in-a-diplomatic-pecha-hfml-tags","title":"Diplomatic edition (OCR or diplomatic input in a diplomatic pecha + HFML tags)","text":"<p>A transcription of a single MS (no attempt to establish \"best\" readings), indicating as far as possible the \"state\" of the text in this manuscript.</p>"},{"location":"data/edition-types/#eclectic-edition","title":"Eclectic edition","text":"<p>A composite text, produced by an editor by taking a line from this MS and another line from that, without the use of a single \"copy\" text.</p>"},{"location":"data/edition-types/#collated-edition-collation-variorum","title":"Collated edition (collation, variorum)","text":"<p>A work that collates all known variants of a text.</p>"},{"location":"data/edition-types/#critical-edition","title":"Critical edition","text":"<p>An attempt to establish a \"best text\" through comparison of various versions (study of \"variants\"); the editor chooses a \"copy text\" (usually that of the most authoritative manuscript) and \"corrects\" it using the variants from other manuscripts.</p> <p>References: https://en.wikipedia.org/wiki/Textual_criticism https://sites.ualberta.ca/~sreimer/ms-course/course/editns.htm https://en.wikipedia.org/wiki/Variorum</p>"},{"location":"data/featured-datasets/","title":"Featured datasets","text":"<ul> <li> <p> Open Parallel Corpus</p> <p>This corpus contains an up-to-date, ever-growing collection of multilingual texts aligned to Tibetan texts (bo) at the sentence-level. It is intended to be used to train an MT model.</p> <p>  Get it on GitHub</p> </li> <li> <p> Vulgate Kangyur</p> <p>This Kangyur was created with OpenPecha's Vulgate Generator, which compares instances of a work and compiles a new version using the most common character at each position in the work.</p> <p>  Get it on GitHub</p> </li> </ul>"},{"location":"data/github/","title":"OpenPecha Data on GitHub","text":"<p>OpenPecha's collection of e-texts on GitHub is contained in more than 14,000 repositories. Each repo contains free open-source Tibetan text files in the OpenPecha format (OPF), and in some cases aligned translations. </p> <p>Most repos contain individual texts, and some contain collections. These collections include corpuses, such those created to train AI and machine translation models, and collections of texts, such as various editions of the Kangyur and Tengyur.</p> <p>Individual repos are given OpenPecha IDs. To see what is indie the repos, visit the pinned repos and browse or search for the texts or collections you are looking for.</p>"},{"location":"data/github/#pinned-repos","title":"Pinned repos","text":"<ul> <li> Works lists all the distinct works on OP Data.</li> <li> Catalog lists all the instances of those works on OP Data. </li> <li> Collections lists OP Data's collections and corpuses.</li> <li> Alignments lists Tibetan texts aligned to translations. </li> </ul> <p>Note Works refer to abstractions of texts, such as the Kangyur, or the Heart Sutra. Instances refers to unique digital editions of works.</p>"},{"location":"data/hfml-specs/","title":"HFML - the human-friendly markup language","text":"<p>Simple markup language for semantic annotations.</p>"},{"location":"data/hfml-specs/#tagset","title":"Tagset","text":"<p>Pagination tags</p> <ul> <li><code>[1]</code></li> <li><code>[1a]</code></li> <li><code>[1a.1]</code></li> </ul> <p>Footnote tags</p> <ul> <li><code>(1)</code></li> <li><code>[100](1)</code></li> </ul>"},{"location":"data/hfml-specs/#pagination-tags","title":"Pagination tags","text":""},{"location":"data/hfml-specs/#1","title":"<code>[1]</code>","text":"<p>Type: Pagination</p> <p>Syntax: <code>[&lt;page number&gt;]</code></p> <p>Use: Mark for modern pagination information in modern book or arabic page numbers in traditional text layout.</p> <p>Text Sample:</p> <p></p> <p><code>[360]</code> <code>\u0f04\u0f05\u0f0d \u0f0d\u0f62\u0f92\u0fb1\u0f0b\u0f42\u0f62\u0f0b\u0f66\u0f90\u0f51\u0f0b\u0f51\u0f74\u0f0d \u0f51\u0fb7\u0f62\u0fa8\u0f0b\u0f59\u0f40\u0fb2\u0f0b\u0f54\u0fb2\u0f0b\u0f56\u0f62\u0f9f\u0f0b\u0f53\u0f0b\u0f66\u0f71\u0f74\u0f0b\u0f4f\u0fb2\u0f0d \u0f56\u0f7c\u0f51\u0f0b\u0f66\u0f90\u0f51\u0f0b\u0f51\u0f74\u0f0d \u0f46\u0f7c\u0f66\u0f0b\u0f40\u0fb1\u0f72\u0f0b\u0f60\u0f41\u0f7c\u0f62\u0f0b\u0f63\u0f7c\u0f0b\u0f62\u0f56\u0f0b\u0f4f\u0f74\u0f0b\u0f56\u0f66\u0f90\u0f7c\u0f62\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f58\u0f51\u0f7c\u0f0d [..]</code> <code>\u0f51\u0fb2\u0f44\u0f0b\u0f66\u0fb2\u0f7c\u0f44\u0f0b\u0f63\u0fb7\u0f74\u0f44\u0f0b\u0f56\u0f0b\u0f62\u0f72\u0f0b\u0f51\u0f42\u0f66\u0f0b\u0f62\u0f92\u0fb1\u0f74\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f42\u0f53\u0f66\u0f0b\u0f53\u0f0b\u0f56\u0f5e\u0f74\u0f42\u0f66\u0f0b\u0f66\u0f7c\u0f0d \u0f0d\u0f51\u0f7a\u0f0b\u0f53\u0f66\u0f0b\u0f56\u0f45\u0f7c\u0f58\u0f0b\u0f63\u0fa1\u0f53\u0f0b\u0f60\u0f51\u0f66\u0f0b\u0f40\u0fb1\u0f72\u0f66\u0f0b\u0f51\u0f42\u0f7a\u0f0b\u0f66\u0fb3\u0f7c\u0f44\u0f0b\u0f63\u0f94\u0f0b\u0f66\u0fa1\u0f7a\u0f0b\u0f62\u0fa3\u0f58\u0f66\u0f0b\u0f56\u0f7c\u0f66\u0f0b\u0f4f\u0f7a\u0f0b\u0f56\u0f40\u0f60\u0f0b[..]</code> <code>\u0f51\u0f44\u0f0b\u0f51\u0f58\u0f53\u0f0b\u0f54\u0f62\u0f0b\u0f60\u0f42\u0fb1\u0f74\u0f62\u0f0b\u0f62\u0f7c\u0f0d \u0f0d\u0f66\u0f7c\u0f0b\u0f66\u0f7c\u0f60\u0f72\u0f0b\u0f66\u0f90\u0fb1\u0f7a\u0f0b\u0f56\u0f7c\u0f0b\u0f62\u0fa3\u0f58\u0f66\u0f0b\u0f53\u0f72\u0f0b\u0f51\u0f7c\u0f53\u0f0b\u0f58\u0f7a\u0f51\u0f0b\u0f54\u0f0b\u0f51\u0f44\u0f0b\u0f63\u0fa1\u0f53\u0f0b\u0f54\u0f60\u0f72\u0f0b\u0f55\u0fb1\u0f72\u0f62\u0f0b\u0f42\u0f44\u0f0b\u0f66\u0f74\u0f0b\u0f51\u0f42\u0f0b\u0f63\u0f74\u0f66\u0f0b\u0f51\u0f74\u0f56\u0f0b\u0f54\u0f0b\u0f51\u0f44\u0f0b\u0f60\u0f56\u0fb2\u0f63\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f66\u0fa1\u0f74\u0f42\u0f0b[..]</code> </p> <p>back to top</p>"},{"location":"data/hfml-specs/#1a","title":"<code>[1a]</code>","text":"<p>Type: Pecha pagination</p> <p>Syntax: <code>[&lt;folio number&gt;&lt;a/b side&gt;]</code></p> <p>Use: Tag for traditional pecha page numbers spelled out in Tibetan on the front side of a folio.</p> <p>Text Sample:</p> <p></p> <p><code>[180b]</code> <code>\u0f04\u0f05\u0f0d \u0f0d\u0f62\u0f92\u0fb1\u0f0b\u0f42\u0f62\u0f0b\u0f66\u0f90\u0f51\u0f0b\u0f51\u0f74\u0f0d \u0f51\u0fb7\u0f62\u0fa8\u0f0b\u0f59\u0f40\u0fb2\u0f0b\u0f54\u0fb2\u0f0b\u0f56\u0f62\u0f9f\u0f0b\u0f53\u0f0b\u0f66\u0f71\u0f74\u0f0b\u0f4f\u0fb2\u0f0d \u0f56\u0f7c\u0f51\u0f0b\u0f66\u0f90\u0f51\u0f0b\u0f51\u0f74\u0f0d \u0f46\u0f7c\u0f66\u0f0b\u0f40\u0fb1\u0f72\u0f0b\u0f60\u0f41\u0f7c\u0f62\u0f0b\u0f63\u0f7c\u0f0b\u0f62\u0f56\u0f0b\u0f4f\u0f74\u0f0b\u0f56\u0f66\u0f90\u0f7c\u0f62\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f58\u0f51\u0f7c\u0f0d [..]</code> <code>\u0f51\u0fb2\u0f44\u0f0b\u0f66\u0fb2\u0f7c\u0f44\u0f0b\u0f63\u0fb7\u0f74\u0f44\u0f0b\u0f56\u0f0b\u0f62\u0f72\u0f0b\u0f51\u0f42\u0f66\u0f0b\u0f62\u0f92\u0fb1\u0f74\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f42\u0f53\u0f66\u0f0b\u0f53\u0f0b\u0f56\u0f5e\u0f74\u0f42\u0f66\u0f0b\u0f66\u0f7c\u0f0d \u0f0d\u0f51\u0f7a\u0f0b\u0f53\u0f66\u0f0b\u0f56\u0f45\u0f7c\u0f58\u0f0b\u0f63\u0fa1\u0f53\u0f0b\u0f60\u0f51\u0f66\u0f0b\u0f40\u0fb1\u0f72\u0f66\u0f0b\u0f51\u0f42\u0f7a\u0f0b\u0f66\u0fb3\u0f7c\u0f44\u0f0b\u0f63\u0f94\u0f0b\u0f66\u0fa1\u0f7a\u0f0b\u0f62\u0fa3\u0f58\u0f66\u0f0b\u0f56\u0f7c\u0f66\u0f0b\u0f4f\u0f7a\u0f0b\u0f56\u0f40\u0f60\u0f0b[..]</code> <code>\u0f51\u0f44\u0f0b\u0f51\u0f58\u0f53\u0f0b\u0f54\u0f62\u0f0b\u0f60\u0f42\u0fb1\u0f74\u0f62\u0f0b\u0f62\u0f7c\u0f0d \u0f0d\u0f66\u0f7c\u0f0b\u0f66\u0f7c\u0f60\u0f72\u0f0b\u0f66\u0f90\u0fb1\u0f7a\u0f0b\u0f56\u0f7c\u0f0b\u0f62\u0fa3\u0f58\u0f66\u0f0b\u0f53\u0f72\u0f0b\u0f51\u0f7c\u0f53\u0f0b\u0f58\u0f7a\u0f51\u0f0b\u0f54\u0f0b\u0f51\u0f44\u0f0b\u0f63\u0fa1\u0f53\u0f0b\u0f54\u0f60\u0f72\u0f0b\u0f55\u0fb1\u0f72\u0f62\u0f0b\u0f42\u0f44\u0f0b\u0f66\u0f74\u0f0b\u0f51\u0f42\u0f0b\u0f63\u0f74\u0f66\u0f0b\u0f51\u0f74\u0f56\u0f0b\u0f54\u0f0b\u0f51\u0f44\u0f0b\u0f60\u0f56\u0fb2\u0f63\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f66\u0fa1\u0f74\u0f42\u0f0b[..]</code> </p> <p>back to top</p>"},{"location":"data/hfml-specs/#1a1","title":"<code>[1a.1]</code>","text":"<p>Type: Pecha pagination</p> <p>Syntax: <code>[&lt;folio number&gt;&lt;side&gt;.&lt;line&gt;]</code></p> <p>Use: Tag for line numbers in traditional pecha layout. The sides are note a/b or \u0f53/\u0f56</p> <p>Text Sample:</p> <p></p> <p><code>[180b]</code> <code>[180b.1]\u0f04\u0f05\u0f0d \u0f0d\u0f62\u0f92\u0fb1\u0f0b\u0f42\u0f62\u0f0b\u0f66\u0f90\u0f51\u0f0b\u0f51\u0f74\u0f0d \u0f51\u0fb7\u0f62\u0fa8\u0f0b\u0f59\u0f40\u0fb2\u0f0b\u0f54\u0fb2\u0f0b\u0f56\u0f62\u0f9f\u0f0b\u0f53\u0f0b\u0f66\u0f71\u0f74\u0f0b\u0f4f\u0fb2\u0f0d \u0f56\u0f7c\u0f51\u0f0b\u0f66\u0f90\u0f51\u0f0b\u0f51\u0f74\u0f0d \u0f46\u0f7c\u0f66\u0f0b\u0f40\u0fb1\u0f72\u0f0b\u0f60\u0f41\u0f7c\u0f62\u0f0b\u0f63\u0f7c\u0f0b\u0f62\u0f56\u0f0b\u0f4f\u0f74\u0f0b\u0f56\u0f66\u0f90\u0f7c\u0f62\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f58\u0f51\u0f7c\u0f0d [..]</code> <code>[180b.2]\u0f51\u0fb2\u0f44\u0f0b\u0f66\u0fb2\u0f7c\u0f44\u0f0b\u0f63\u0fb7\u0f74\u0f44\u0f0b\u0f56\u0f0b\u0f62\u0f72\u0f0b\u0f51\u0f42\u0f66\u0f0b\u0f62\u0f92\u0fb1\u0f74\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f42\u0f53\u0f66\u0f0b\u0f53\u0f0b\u0f56\u0f5e\u0f74\u0f42\u0f66\u0f0b\u0f66\u0f7c\u0f0d \u0f0d\u0f51\u0f7a\u0f0b\u0f53\u0f66\u0f0b\u0f56\u0f45\u0f7c\u0f58\u0f0b\u0f63\u0fa1\u0f53\u0f0b\u0f60\u0f51\u0f66\u0f0b\u0f40\u0fb1\u0f72\u0f66\u0f0b\u0f51\u0f42\u0f7a\u0f0b\u0f66\u0fb3\u0f7c\u0f44\u0f0b\u0f63\u0f94\u0f0b\u0f66\u0fa1\u0f7a\u0f0b\u0f62\u0fa3\u0f58\u0f66\u0f0b\u0f56\u0f7c\u0f66\u0f0b\u0f4f\u0f7a\u0f0b\u0f56\u0f40\u0f60\u0f0b[..]</code> <code>[180b.3]\u0f51\u0f44\u0f0b\u0f51\u0f58\u0f53\u0f0b\u0f54\u0f62\u0f0b\u0f60\u0f42\u0fb1\u0f74\u0f62\u0f0b\u0f62\u0f7c\u0f0d \u0f0d\u0f66\u0f7c\u0f0b\u0f66\u0f7c\u0f60\u0f72\u0f0b\u0f66\u0f90\u0fb1\u0f7a\u0f0b\u0f56\u0f7c\u0f0b\u0f62\u0fa3\u0f58\u0f66\u0f0b\u0f53\u0f72\u0f0b\u0f51\u0f7c\u0f53\u0f0b\u0f58\u0f7a\u0f51\u0f0b\u0f54\u0f0b\u0f51\u0f44\u0f0b\u0f63\u0fa1\u0f53\u0f0b\u0f54\u0f60\u0f72\u0f0b\u0f55\u0fb1\u0f72\u0f62\u0f0b\u0f42\u0f44\u0f0b\u0f66\u0f74\u0f0b\u0f51\u0f42\u0f0b\u0f63\u0f74\u0f66\u0f0b\u0f51\u0f74\u0f56\u0f0b\u0f54\u0f0b\u0f51\u0f44\u0f0b\u0f60\u0f56\u0fb2\u0f63\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f66\u0fa1\u0f74\u0f42\u0f0b[..]</code> </p> <p>back to top</p>"},{"location":"data/hfml-specs/#toc-tags","title":"TOC tags","text":"<p><code>{T###}</code> text ID</p> <p><code>{T###-##}</code> section/chapter ID</p> <p>back to top</p>"},{"location":"data/hfml-specs/#footnote-tags","title":"Footnote tags","text":"<p><code>[^##]</code> inline note marker <code>[^##]:</code> note content prefix</p>"},{"location":"data/hfml-specs/#endnote-tags-without-page-reference","title":"Endnote tags without page reference","text":"<p><code>(##)</code> endnote marker <code>(##):</code> endnote content prefix</p> <p><code>[###](##):</code> endnote content prefix</p>"},{"location":"data/hfml-specs/#1_1","title":"<code>(1)</code>","text":"<p>Type: Note maker</p> <p>Syntax: <code>(&lt;note number&gt;)]</code></p> <p>Use: Marker for both footnotes and endnotes</p> <p>Text Sample:</p> <p></p> <p><code>[517]</code> <code>\u0f04\u0f05\u0f0d \u0f0d\u0f46\u0f7c\u0f66\u0f0b\u0f40\u0fb1\u0f72\u0f0b\u0f60\u0f41\u0f7c\u0f62\u0f0b\u0f63\u0f7c\u0f0b\u0f62\u0f56\u0f0b\u0f4f\u0f74\u0f0b\u0f56\u0f66\u0f90\u0f7c\u0f62\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f58\u0f51\u0f7c\u0f0d(1)</code></p> <p>back to top</p>"},{"location":"data/hfml-specs/#1001","title":"<code>[100](1):</code>","text":"<p>Type: Endnote content prefix</p> <p>Syntax: <code>[&lt;page reference&gt;](&lt;note number&gt;)]</code></p> <p>Use: Marker for the content of endnotes located at the end of texts.</p> <p>Text Sample:</p> <p></p>"},{"location":"data/hfml-specs/#reference","title":"Reference","text":"HFML tagset from 2021    # HFML - the human-friendly markup language  Simple markup language for semantic annotations.  ## HFML Tags  ### IE tags  | \u0f56\u0f7c\u0f51\u0f0d bo           | \u0f68\u0f72\u0f53\u0f0d en                       | \u0f62\u0f9f\u0f42\u0f66\u0f0d tag        | \u0f51\u0f54\u0f7a\u0f62\u0f0b\u0f56\u0f62\u0f97\u0f7c\u0f51\u0f0d Example                                                                                           | |------------------|------------------------------|-----------------|------------------------------------------------------------------------------------------------------------| | \u0f58\u0f5a\u0f7c\u0f53\u0f0b\u0f56\u0fb1\u0f0d           | what is defined              |       |                                                                                                     | | \u0f58\u0f5a\u0f53\u0f0b\u0f49\u0f72\u0f51\u0f0d          | definition                   |       |                                                                                            | | \u0f58\u0f5a\u0f53\u0f0b\u0f42\u0f5e\u0f72\u0f0d          | instance                     |       |                                                                                                            | | \u0f51\u0f56\u0fb1\u0f7a\u0f0b\u0f42\u0f5e\u0f72\u0f0d           | what is enumerated           |       | \u0f51\u0f56\u0fb1\u0f7a\u0f0b\u0f53\u0f0b\u0f42\u0f49\u0f72\u0f66\u0f0b\u0f61\u0f7c\u0f51\u0f0b\u0f51\u0f7a\u0f0d                                                                       | | \u0f51\u0f56\u0fb1\u0f7a\u0f0b\u0f56\u0f0d            | enumeration                  |       | \u0f42\u0f49\u0f72\u0f66\u0f0b\u0f61\u0f7c\u0f51\u0f0b\u0f54\u0f60\u0f72\u0f0b\u0f55\u0fb1\u0f72\u0f62\u0f0d                                                  | | \u0f66\u0f92\u0fb2\u0f0b\u0f56\u0f64\u0f51\u0f0d           | word part explanation        |       |                                                                                                            | | \u0f66\u0f92\u0fb2\u0f0b\u0f42\u0f5e\u0f72\u0f0d            | what is explained            |       |                                                                                                            | | \u0f63\u0f74\u0f44\u0f0b\u0f5a\u0f72\u0f42            | citation                     |      | \u0f5e\u0f7a\u0f66\u0f0b\u0f42\u0f66\u0f74\u0f44\u0f66\u0f0b\u0f54\u0f0b\u0f61\u0f72\u0f53\u0f0b\u0f54\u0f60\u0f72\u0f0b\u0f55\u0fb1\u0f72\u0f62\u0f0d                                | | \u0f63\u0f74\u0f44\u0f0b\u0f41\u0f74\u0f44\u0f66\u0f0d          | source                       |       |                                                                                           | | \u0f58\u0f5b\u0f51\u0f0b\u0f56\u0fb1\u0f44\u0f0b\u0f0d         | colophon                     |       |    | | \u0f56\u0f66\u0f92\u0fb1\u0f74\u0f62\u0f0b\u0f56\u0fb1\u0f44\u0f0b\u0f0d         | translation statement        |       |                                                                                                            | | \u0f42\u0f7c\u0f0b\u0f56\u0fb1\u0f0d             | what is explained            |      |                                                                                                            | | \u0f42\u0f7c\u0f0b\u0f51\u0f7c\u0f53\u0f0d            | meaning                      |     |                                                                                                            | | \u0f51\u0f54\u0f7a\u0f62\u0f0b\u0f56\u0f62\u0f97\u0f7c\u0f51\u0f0d         | example                      |    |                                                                                                            | | \u0f60\u0f42\u0fb2\u0f7a\u0f63\u0f0b\u0f42\u0f5e\u0f72\u0f0d          | what is explained            |    | \u0f5e\u0f7a\u0f66\u0f0b\u0f56\u0fb1\u0f0b\u0f56\u0f0b\u0f53\u0f72\u0f0b                                                                                          | | \u0f60\u0f42\u0fb2\u0f7a\u0f63\u0f0b\u0f56\u0f64\u0f51\u0f0d         | emplanation                  |   | \u0f53\u0f72\u0f0b                                                                                    | | \u0f66\u0f90\u0f56\u0f66\u0f0b\u0f56\u0f66\u0f9f\u0f74\u0f53\u0f0b\u0f60\u0f42\u0fb2\u0f7a\u0f63\u0f0b\u0f56\u0f64\u0f51\u0f0d | context-specific explanation |  |                                                   | | \u0f60\u0f47\u0f74\u0f42\u0f0b\u0f61\u0f74\u0f63\u0f0d          | agreement female             |      | \u0f61\u0f72\u0f53\u0f0b\u0f54\u0f60\u0f72\u0f0b\u0f55\u0fb1\u0f72\u0f62\u0f0b\u0f62\u0f7c\u0f0d \u0f0d | | \u0f60\u0f47\u0f74\u0f42\u0f0b\u0f56\u0fb1\u0f0d           | agreement male               |     | \u0f66\u0f9f\u0f7a\u0f0d                                                                          | | \u0f58\u0f72\u0f0b\u0f60\u0f47\u0f74\u0f42\u0f0b\u0f66\u0f60\u0f72\u0f0b\u0f61\u0f74\u0f63\u0f0d     | illegal agreement female     |          |                                                                                                            | | \u0f58\u0f72\u0f0b\u0f60\u0f47\u0f74\u0f42\u0f0b\u0f66\u0f0d         | illegal agreement male       |         |                                                                                                            | | \u0f66\u0f0b\u0f56\u0f45\u0f51\u0f0d           | outline                      |     |                                                                                                            | | \u0f66\u0f0b\u0f56\u0f45\u0f51\u0f0b\u0f40\u0fb1\u0f72\u0f0b\u0f51\u0f56\u0fb1\u0f7a\u0f0b\u0f42\u0f5e\u0f72\u0f0d   | outline node                 |     |                                                                                                            | | \u0f66\u0f0b\u0f56\u0f45\u0f51\u0f0b\u0f40\u0fb1\u0f72\u0f0b\u0f53\u0f44\u0f0b\u0f42\u0f66\u0f7a\u0f66\u0f0d  | outline branches             |    |                                                                                                            | | \u0f44\u0f7c\u0f66\u0f0b\u0f60\u0f5b\u0f72\u0f53\u0f0b\u0f56\u0fb1\u0f0d        | what is identified           |     |                                                                                                            | | \u0f44\u0f7c\u0f66\u0f0b\u0f60\u0f5b\u0f72\u0f53\u0f0b\u0f56\u0fb1\u0f7a\u0f51\u0f0d       | identification               |    |                                                                                                            | | \u0f62\u0fa9\u0f0b\u0f56\u0f0d             | root text                    |    | \u0f0d\u0f45\u0f7a\u0f66\u0f0b\u0f42\u0f66\u0f74\u0f44\u0f66\u0f0b\u0f54\u0f0b\u0f63\u0f9f\u0f62\u0f0d                             | | \u0f60\u0f42\u0fb2\u0f7a\u0f63\u0f0b\u0f56\u0f0d           | commentary                   |    |                                                                                                            |   ### Source text pagination tags `[1a]` folio, side  `[1a.1]` folio, side, line  ### TOC tags `{T###}` text ID  `{T###-##}` section/chapter ID  ### Layout tags  | \u0f56\u0f7c\u0f51\u0f0d bo         | \u0f68\u0f72\u0f53\u0f0d en                                                  | \u0f62\u0f9f\u0f42\u0f66\u0f0d tag    | \u0f51\u0f54\u0f7a\u0f62\u0f0b\u0f56\u0f62\u0f97\u0f7c\u0f51\u0f0d Example                                                                                           | | -------------- | ------------------------------------------------------- | ----------- | ---------------------------------------------------------------------------------------------------------- | | \u0f61\u0f72\u0f42\u0f0b\u0f46\u0f74\u0f44\u0f0b\u0f0d        | contains smaller text size                              | \\     | \\ | | \u0f58\u0f5b\u0f51\u0f0b\u0f54\u0f0b\u0f54\u0f7c\u0f0d       | Name\\ of an author, personal or corporate, of a work. | \\    | \\                                                             | | \u0f51\u0f54\u0f7a\u0f0b\u0f46\u0f60\u0f72\u0f0b\u0f58\u0f5a\u0f53\u0f0b\u0f56\u0fb1\u0f44\u0f0b\u0f0d | contains pecha title                                    | \\ | \\                                                                    | | \u0f54\u0f7c\u0f0b\u0f4f\u0f72\u0f60\u0f72\u0f0b\u0f58\u0f5a\u0f53\u0f0b\u0f56\u0fb1\u0f44\u0f0b\u0f0d  | contains poti title                                     | \\ | \\                                                                                 | | \u0f63\u0f7a\u0f60\u0f74\u0f0b\u0f61\u0f72\u0f0b\u0f58\u0f5a\u0f53\u0f0b\u0f56\u0fb1\u0f44\u0f0b\u0f0d  | contains chapter title                                  | \\ | \\                                                                              |   `#\\` denotes the peydurma notes. Eg #\u0f51\u0f42\u0f0b\u0f60\u0f47\u0f7c\u0f42\u0f0b\u0f54\u0f62\u0f0b\u0f56\u0fb1\u0f7a\u0f51\u0f0b\u0f54\u0f66\u0f0b\u0f56\u0f51\u0f42\u0f0b\u0f45\u0f42\u0f0b\u0f62\u0f7a\u0f0b\u0f5e\u0f72\u0f42\u0f0b\u0f56\u0f45\u0f7c\u0f58\u0f0b\u0f63\u0fa1\u0f53\u0f0b\u0f60\u0f51\u0f66\u0f0b\u0f63\u0f0b\u0f56\u0f63\u0f9f\u0f0b\u0f56\u0f0b\u0f51\u0f44\u0f0b\u0f56\u0f66\u0f99\u0f7a\u0f53\u0f0b\u0f56\u0f40\u0f74\u0f62\u0f0b\u0f56\u0fb1\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f55\u0fb1\u0f72\u0f62\u0f0b\u0f60\u0f51\u0f7c\u0f44\u0f0b\u0f56\u0f0b\u0f66\u0f94\u0f66\u0f0b\u0f40\u0fb1\u0f72\u0f66\u0f0d  ### Spell-checking tags  `\\` potential error, correction suggestion  ### Critical aparatus `[ ]` uncertain reading  `\\&lt;* \\&gt;`  editorial restoration of lost text  `\u27e8* \u27e9`  editorial addition of omitted text  `\u27ea \u27eb` scribal insertion  `{ }` editorial deletion of redundant text  `{{ }}` scribal deletion  `///` textual loss at left or right edge of support  ## Notes  - we don't encode lines as annotations and generate them on the fly from the pagination layer and line returns in the base  ## Sources - [gandhari.org]\\ - [esukhia/derge-kangyur]\\ &lt;\\details&gt;   `[517](1) \u0f58\u0f5a\u0f53\u0f0b\u0f56\u0fb1\u0f44\u0f0b\u0f60\u0f51\u0f72\u0f0b\u0f46\u0f7c\u0f66\u0f0b\u0f5a\u0f53\u0f0b\u0f60\u0f51\u0f72\u0f60\u0f72\u0f0b\u0f58\u0f5a\u0f53\u0f0b\u0f56\u0f7c\u0f51\u0f0b\u0f66\u0f90\u0f51\u0f0b\u0f51\u0f74\u0f0b\u0f66\u0fa8\u0f7c\u0f66\u0f0b\u0f54\u0f0b[..]`   `\u0f58\u0f51\u0f7c\u0f0b\u0f5a\u0f53\u0f0b\u0f56\u0f45\u0f74\u0f0b\u0f42\u0f66\u0f74\u0f58\u0f0b\u0f66\u0fa3\u0f62\u0f0b\u0f50\u0f44\u0f0b\u0f54\u0f62\u0f0b\u0f51\u0f74\u0f0b\u0f58\u0f51\u0f7c\u0f0b\u0f68\u0f0b\u0f54\u0f60\u0f72\u0f0b\u0f42\u0f64\u0f58\u0f0b\u0f51\u0f74\u0f0b[..]`  [back to top](https://github.com/OpenPecha/hfml#tagset)  ---  ### Spell-checking tags  **Type:** potential error, correction suggestion  **Syntax:** `(error,suggestion)`\u00a0  **Use:** Marker for the content of endnotes located at the end of texts.  **Text Sample:**  ---  ### Critical aparatus  `[? ]` uncertain reading  `\\&lt;* \\&gt;` editorial restoration of lost text  `\u27e8* \u27e9` editorial addition of omitted text  `\u27ea \u27eb` scribal insertion  `{ }` editorial deletion of redundant text  `{{ }}` scribal deletion  `///` textual loss at left or right edge of support  ### Layout tags  | \u0f56\u0f7c\u0f51\u0f0d bo | \u0f68\u0f72\u0f53\u0f0d en | \u0f62\u0f9f\u0f42\u0f66\u0f0d tag | \u0f51\u0f54\u0f7a\u0f62\u0f0b\u0f56\u0f62\u0f97\u0f7c\u0f51\u0f0d Example | | --- | --- | --- | --- | | \u0f61\u0f72\u0f42\u0f0b\u0f46\u0f74\u0f44\u0f0b\u0f0d | contains smaller text size | \\ | \\ | | \u0f58\u0f5b\u0f51\u0f0b\u0f54\u0f0b\u0f54\u0f7c\u0f0d | Name\\ of an author, personal or corporate, of a work. | \\ | \\ | | \u0f51\u0f54\u0f7a\u0f0b\u0f46\u0f60\u0f72\u0f0b\u0f58\u0f5a\u0f53\u0f0b\u0f56\u0fb1\u0f44\u0f0b\u0f0d | contains pecha title | \\ | \\ | | \u0f54\u0f7c\u0f0b\u0f4f\u0f72\u0f60\u0f72\u0f0b\u0f58\u0f5a\u0f53\u0f0b\u0f56\u0fb1\u0f44\u0f0b\u0f0d | contains poti title | \\ | \\ | | \u0f63\u0f7a\u0f60\u0f74\u0f0b\u0f61\u0f72\u0f0b\u0f58\u0f5a\u0f53\u0f0b\u0f56\u0fb1\u0f44\u0f0b\u0f0d | contains chapter title | \\ | \\ |  ## Notes  *   we don't encode lines as annotations and generate them on the fly from the pagination layer and line returns in the base  ## Sources  *   https://ubsicap.github.io/usfm/ *   \\[gandhari.org\\]\\ *   \\[esukhia/derge-kangyur\\]\\ *   https://www.markdownguide.org/extended-syntax/#fn:bignote"},{"location":"data/hfml/","title":"HFML","text":"<p>HFML stands for The human-friendly markup language</p>"},{"location":"data/hfml/#import-hfml-files","title":"Import HFML Files","text":"<p>Here is the hfml file kangyur_01 used in following code snippet.</p> <pre><code>from pathlib import Path\n\nfrom openpecha.formatters import HFMLFormatter\n\nhfml_fn = Path(\"tests\") / \"formatters\" / \"hfml\" / \"data\" / \"kangyur_01.txt\"\nm_text = hfml_fn.read_text(encoding='utf-8')\n\nformatter = HFMLFormatter()\n\ntext = formatter.text_preprocess(m_text)\nformatter.build_layers(text, len([text]))\nresult = formatter.get_base_text()\n</code></pre>"},{"location":"data/hfml/#tagset","title":"Tagset","text":""},{"location":"data/hfml/#pagination-tags","title":"Pagination tags","text":""},{"location":"data/hfml/#1","title":"<code>[1]</code>","text":"<p>Type: Pagination</p> <p>Syntax: <code>[&lt;page number&gt;]</code></p> <p>Use: Mark for modern pagination information in modern book or arabic page numbers in traditional text layout.</p> <p>Text Sample:</p> <p></p> <p><code>[360]</code> <code>\u0f04\u0f05\u0f0d \u0f0d\u0f62\u0f92\u0fb1\u0f0b\u0f42\u0f62\u0f0b\u0f66\u0f90\u0f51\u0f0b\u0f51\u0f74\u0f0d \u0f51\u0fb7\u0f62\u0fa8\u0f0b\u0f59\u0f40\u0fb2\u0f0b\u0f54\u0fb2\u0f0b\u0f56\u0f62\u0f9f\u0f0b\u0f53\u0f0b\u0f66\u0f71\u0f74\u0f0b\u0f4f\u0fb2\u0f0d \u0f56\u0f7c\u0f51\u0f0b\u0f66\u0f90\u0f51\u0f0b\u0f51\u0f74\u0f0d \u0f46\u0f7c\u0f66\u0f0b\u0f40\u0fb1\u0f72\u0f0b\u0f60\u0f41\u0f7c\u0f62\u0f0b\u0f63\u0f7c\u0f0b\u0f62\u0f56\u0f0b\u0f4f\u0f74\u0f0b\u0f56\u0f66\u0f90\u0f7c\u0f62\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f58\u0f51\u0f7c\u0f0d [..]</code> <code>\u0f51\u0fb2\u0f44\u0f0b\u0f66\u0fb2\u0f7c\u0f44\u0f0b\u0f63\u0fb7\u0f74\u0f44\u0f0b\u0f56\u0f0b\u0f62\u0f72\u0f0b\u0f51\u0f42\u0f66\u0f0b\u0f62\u0f92\u0fb1\u0f74\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f42\u0f53\u0f66\u0f0b\u0f53\u0f0b\u0f56\u0f5e\u0f74\u0f42\u0f66\u0f0b\u0f66\u0f7c\u0f0d \u0f0d\u0f51\u0f7a\u0f0b\u0f53\u0f66\u0f0b\u0f56\u0f45\u0f7c\u0f58\u0f0b\u0f63\u0fa1\u0f53\u0f0b\u0f60\u0f51\u0f66\u0f0b\u0f40\u0fb1\u0f72\u0f66\u0f0b\u0f51\u0f42\u0f7a\u0f0b\u0f66\u0fb3\u0f7c\u0f44\u0f0b\u0f63\u0f94\u0f0b\u0f66\u0fa1\u0f7a\u0f0b\u0f62\u0fa3\u0f58\u0f66\u0f0b\u0f56\u0f7c\u0f66\u0f0b\u0f4f\u0f7a\u0f0b\u0f56\u0f40\u0f60\u0f0b[..]</code> <code>\u0f51\u0f44\u0f0b\u0f51\u0f58\u0f53\u0f0b\u0f54\u0f62\u0f0b\u0f60\u0f42\u0fb1\u0f74\u0f62\u0f0b\u0f62\u0f7c\u0f0d \u0f0d\u0f66\u0f7c\u0f0b\u0f66\u0f7c\u0f60\u0f72\u0f0b\u0f66\u0f90\u0fb1\u0f7a\u0f0b\u0f56\u0f7c\u0f0b\u0f62\u0fa3\u0f58\u0f66\u0f0b\u0f53\u0f72\u0f0b\u0f51\u0f7c\u0f53\u0f0b\u0f58\u0f7a\u0f51\u0f0b\u0f54\u0f0b\u0f51\u0f44\u0f0b\u0f63\u0fa1\u0f53\u0f0b\u0f54\u0f60\u0f72\u0f0b\u0f55\u0fb1\u0f72\u0f62\u0f0b\u0f42\u0f44\u0f0b\u0f66\u0f74\u0f0b\u0f51\u0f42\u0f0b\u0f63\u0f74\u0f66\u0f0b\u0f51\u0f74\u0f56\u0f0b\u0f54\u0f0b\u0f51\u0f44\u0f0b\u0f60\u0f56\u0fb2\u0f63\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f66\u0fa1\u0f74\u0f42\u0f0b[..]</code> </p>"},{"location":"data/hfml/#1a","title":"<code>[1a]</code>","text":"<p>Type: Pecha folio pagination</p> <p>Syntax: <code>[&lt;page number&gt;&lt;a/b side&gt;]</code></p> <p>Use: Tag for traditional pecha page numbers spelled out in Tibetan on the front side of a folio.</p> <p>Text Sample:</p> <p></p> <p><code>[180b]</code> <code>\u0f04\u0f05\u0f0d \u0f0d\u0f62\u0f92\u0fb1\u0f0b\u0f42\u0f62\u0f0b\u0f66\u0f90\u0f51\u0f0b\u0f51\u0f74\u0f0d \u0f51\u0fb7\u0f62\u0fa8\u0f0b\u0f59\u0f40\u0fb2\u0f0b\u0f54\u0fb2\u0f0b\u0f56\u0f62\u0f9f\u0f0b\u0f53\u0f0b\u0f66\u0f71\u0f74\u0f0b\u0f4f\u0fb2\u0f0d \u0f56\u0f7c\u0f51\u0f0b\u0f66\u0f90\u0f51\u0f0b\u0f51\u0f74\u0f0d \u0f46\u0f7c\u0f66\u0f0b\u0f40\u0fb1\u0f72\u0f0b\u0f60\u0f41\u0f7c\u0f62\u0f0b\u0f63\u0f7c\u0f0b\u0f62\u0f56\u0f0b\u0f4f\u0f74\u0f0b\u0f56\u0f66\u0f90\u0f7c\u0f62\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f58\u0f51\u0f7c\u0f0d [..]</code> <code>\u0f51\u0fb2\u0f44\u0f0b\u0f66\u0fb2\u0f7c\u0f44\u0f0b\u0f63\u0fb7\u0f74\u0f44\u0f0b\u0f56\u0f0b\u0f62\u0f72\u0f0b\u0f51\u0f42\u0f66\u0f0b\u0f62\u0f92\u0fb1\u0f74\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f42\u0f53\u0f66\u0f0b\u0f53\u0f0b\u0f56\u0f5e\u0f74\u0f42\u0f66\u0f0b\u0f66\u0f7c\u0f0d \u0f0d\u0f51\u0f7a\u0f0b\u0f53\u0f66\u0f0b\u0f56\u0f45\u0f7c\u0f58\u0f0b\u0f63\u0fa1\u0f53\u0f0b\u0f60\u0f51\u0f66\u0f0b\u0f40\u0fb1\u0f72\u0f66\u0f0b\u0f51\u0f42\u0f7a\u0f0b\u0f66\u0fb3\u0f7c\u0f44\u0f0b\u0f63\u0f94\u0f0b\u0f66\u0fa1\u0f7a\u0f0b\u0f62\u0fa3\u0f58\u0f66\u0f0b\u0f56\u0f7c\u0f66\u0f0b\u0f4f\u0f7a\u0f0b\u0f56\u0f40\u0f60\u0f0b[..]</code> <code>\u0f51\u0f44\u0f0b\u0f51\u0f58\u0f53\u0f0b\u0f54\u0f62\u0f0b\u0f60\u0f42\u0fb1\u0f74\u0f62\u0f0b\u0f62\u0f7c\u0f0d \u0f0d\u0f66\u0f7c\u0f0b\u0f66\u0f7c\u0f60\u0f72\u0f0b\u0f66\u0f90\u0fb1\u0f7a\u0f0b\u0f56\u0f7c\u0f0b\u0f62\u0fa3\u0f58\u0f66\u0f0b\u0f53\u0f72\u0f0b\u0f51\u0f7c\u0f53\u0f0b\u0f58\u0f7a\u0f51\u0f0b\u0f54\u0f0b\u0f51\u0f44\u0f0b\u0f63\u0fa1\u0f53\u0f0b\u0f54\u0f60\u0f72\u0f0b\u0f55\u0fb1\u0f72\u0f62\u0f0b\u0f42\u0f44\u0f0b\u0f66\u0f74\u0f0b\u0f51\u0f42\u0f0b\u0f63\u0f74\u0f66\u0f0b\u0f51\u0f74\u0f56\u0f0b\u0f54\u0f0b\u0f51\u0f44\u0f0b\u0f60\u0f56\u0fb2\u0f63\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f66\u0fa1\u0f74\u0f42\u0f0b[..]</code> </p> <p>back to top</p>"},{"location":"data/hfml/#1a1","title":"<code>[1a.1]</code>","text":"<p>Type: Pecha pagination</p> <p>Syntax: <code>[&lt;page number&gt;&lt;a/b side&gt;.&lt;line number&gt;]</code></p> <p>Use: Tag for line numbers in traditional pecha layout.</p> <p>Text Sample:</p> <p></p> <p><code>[180b]</code> <code>[180b.1]\u0f04\u0f05\u0f0d \u0f0d\u0f62\u0f92\u0fb1\u0f0b\u0f42\u0f62\u0f0b\u0f66\u0f90\u0f51\u0f0b\u0f51\u0f74\u0f0d \u0f51\u0fb7\u0f62\u0fa8\u0f0b\u0f59\u0f40\u0fb2\u0f0b\u0f54\u0fb2\u0f0b\u0f56\u0f62\u0f9f\u0f0b\u0f53\u0f0b\u0f66\u0f71\u0f74\u0f0b\u0f4f\u0fb2\u0f0d \u0f56\u0f7c\u0f51\u0f0b\u0f66\u0f90\u0f51\u0f0b\u0f51\u0f74\u0f0d \u0f46\u0f7c\u0f66\u0f0b\u0f40\u0fb1\u0f72\u0f0b\u0f60\u0f41\u0f7c\u0f62\u0f0b\u0f63\u0f7c\u0f0b\u0f62\u0f56\u0f0b\u0f4f\u0f74\u0f0b\u0f56\u0f66\u0f90\u0f7c\u0f62\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f58\u0f51\u0f7c\u0f0d [..]</code> <code>[180b.2]\u0f51\u0fb2\u0f44\u0f0b\u0f66\u0fb2\u0f7c\u0f44\u0f0b\u0f63\u0fb7\u0f74\u0f44\u0f0b\u0f56\u0f0b\u0f62\u0f72\u0f0b\u0f51\u0f42\u0f66\u0f0b\u0f62\u0f92\u0fb1\u0f74\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f42\u0f53\u0f66\u0f0b\u0f53\u0f0b\u0f56\u0f5e\u0f74\u0f42\u0f66\u0f0b\u0f66\u0f7c\u0f0d \u0f0d\u0f51\u0f7a\u0f0b\u0f53\u0f66\u0f0b\u0f56\u0f45\u0f7c\u0f58\u0f0b\u0f63\u0fa1\u0f53\u0f0b\u0f60\u0f51\u0f66\u0f0b\u0f40\u0fb1\u0f72\u0f66\u0f0b\u0f51\u0f42\u0f7a\u0f0b\u0f66\u0fb3\u0f7c\u0f44\u0f0b\u0f63\u0f94\u0f0b\u0f66\u0fa1\u0f7a\u0f0b\u0f62\u0fa3\u0f58\u0f66\u0f0b\u0f56\u0f7c\u0f66\u0f0b\u0f4f\u0f7a\u0f0b\u0f56\u0f40\u0f60\u0f0b[..]</code> <code>[180b.3]\u0f51\u0f44\u0f0b\u0f51\u0f58\u0f53\u0f0b\u0f54\u0f62\u0f0b\u0f60\u0f42\u0fb1\u0f74\u0f62\u0f0b\u0f62\u0f7c\u0f0d \u0f0d\u0f66\u0f7c\u0f0b\u0f66\u0f7c\u0f60\u0f72\u0f0b\u0f66\u0f90\u0fb1\u0f7a\u0f0b\u0f56\u0f7c\u0f0b\u0f62\u0fa3\u0f58\u0f66\u0f0b\u0f53\u0f72\u0f0b\u0f51\u0f7c\u0f53\u0f0b\u0f58\u0f7a\u0f51\u0f0b\u0f54\u0f0b\u0f51\u0f44\u0f0b\u0f63\u0fa1\u0f53\u0f0b\u0f54\u0f60\u0f72\u0f0b\u0f55\u0fb1\u0f72\u0f62\u0f0b\u0f42\u0f44\u0f0b\u0f66\u0f74\u0f0b\u0f51\u0f42\u0f0b\u0f63\u0f74\u0f66\u0f0b\u0f51\u0f74\u0f56\u0f0b\u0f54\u0f0b\u0f51\u0f44\u0f0b\u0f60\u0f56\u0fb2\u0f63\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f66\u0fa1\u0f74\u0f42\u0f0b[..]</code> </p> <p>back to top</p>"},{"location":"data/hfml/#toc-tags","title":"TOC tags","text":"<p><code>{T###}</code> text ID</p> <p><code>{T###-##}</code> section/chapter ID</p> <p>back to top</p>"},{"location":"data/hfml/#footnote-tags","title":"Footnote tags","text":"<p><code>[^##]</code> inline note marker <code>[^##]:</code> note content prefix</p>"},{"location":"data/hfml/#endnote-tags-without-page-reference","title":"Endnote tags without page reference","text":"<p><code>(##)</code> endnote marker <code>(##):</code> endnote content prefix</p> <p><code>[###](##):</code> endnote content prefix</p>"},{"location":"data/hfml/#1_1","title":"<code>(1)</code>","text":"<p>Type: Note maker</p> <p>Syntax: <code>(&lt;note number&gt;)]</code></p> <p>Use: Marker for both footnotes and endnotes</p> <p>Text Sample:</p> <p></p> <p><code>[517]</code> <code>\u0f04\u0f05\u0f0d \u0f0d\u0f46\u0f7c\u0f66\u0f0b\u0f40\u0fb1\u0f72\u0f0b\u0f60\u0f41\u0f7c\u0f62\u0f0b\u0f63\u0f7c\u0f0b\u0f62\u0f56\u0f0b\u0f4f\u0f74\u0f0b\u0f56\u0f66\u0f90\u0f7c\u0f62\u0f0b\u0f56\u0f60\u0f72\u0f0b\u0f58\u0f51\u0f7c\u0f0d(1)</code></p> <p>back to top</p>"},{"location":"data/hfml/#1001","title":"<code>[100](1):</code>","text":"<p>Type: Endnote content prefix</p> <p>Syntax: <code>[&lt;page reference&gt;](&lt;note number&gt;)]</code></p> <p>Use: Marker for the content of endnotes located at the end of texts.</p> <p>Text Sample:</p> <p></p> <p><code>[517](1) \u0f58\u0f5a\u0f53\u0f0b\u0f56\u0fb1\u0f44\u0f0b\u0f60\u0f51\u0f72\u0f0b\u0f46\u0f7c\u0f66\u0f0b\u0f5a\u0f53\u0f0b\u0f60\u0f51\u0f72\u0f60\u0f72\u0f0b\u0f58\u0f5a\u0f53\u0f0b\u0f56\u0f7c\u0f51\u0f0b\u0f66\u0f90\u0f51\u0f0b\u0f51\u0f74\u0f0b\u0f66\u0fa8\u0f7c\u0f66\u0f0b\u0f54\u0f0b[..]</code> <code>\u0f58\u0f51\u0f7c\u0f0b\u0f5a\u0f53\u0f0b\u0f56\u0f45\u0f74\u0f0b\u0f42\u0f66\u0f74\u0f58\u0f0b\u0f66\u0fa3\u0f62\u0f0b\u0f50\u0f44\u0f0b\u0f54\u0f62\u0f0b\u0f51\u0f74\u0f0b\u0f58\u0f51\u0f7c\u0f0b\u0f68\u0f0b\u0f54\u0f60\u0f72\u0f0b\u0f42\u0f64\u0f58\u0f0b\u0f51\u0f74\u0f0b[..]</code></p> <p>back to top</p>"},{"location":"data/hfml/#spell-checking-tags","title":"Spell-checking tags","text":"<p>Type: potential error, correction suggestion</p> <p>Syntax: <code>(error,suggestion)</code> </p> <p>Use: Marker for the content of endnotes located at the end of texts.</p> <p>Text Sample:</p>"},{"location":"data/hfml/#critical-aparatus","title":"Critical aparatus","text":"<p><code>[? ]</code> uncertain reading</p> <p><code>\\&lt;* \\&gt;</code> editorial restoration of lost text</p> <p><code>\u27e8* \u27e9</code> editorial addition of omitted text</p> <p><code>\u27ea \u27eb</code> scribal insertion</p> <p><code>{ }</code> editorial deletion of redundant text</p> <p><code>{{ }}</code> scribal deletion</p> <p><code>///</code> textual loss at left or right edge of support</p>"},{"location":"data/hfml/#layout-tags","title":"Layout tags","text":"\u0f56\u0f7c\u0f51\u0f0d bo \u0f68\u0f72\u0f53\u0f0d en \u0f62\u0f9f\u0f42\u0f66\u0f0d tag \u0f51\u0f54\u0f7a\u0f62\u0f0b\u0f56\u0f62\u0f97\u0f7c\u0f51\u0f0d Example \u0f61\u0f72\u0f42\u0f0b\u0f46\u0f74\u0f44\u0f0b\u0f0d contains smaller text size \\ \\ \u0f58\u0f5b\u0f51\u0f0b\u0f54\u0f0b\u0f54\u0f7c\u0f0d Name\\ of an author, personal or corporate, of a work. \\ \\ \u0f51\u0f54\u0f7a\u0f0b\u0f46\u0f60\u0f72\u0f0b\u0f58\u0f5a\u0f53\u0f0b\u0f56\u0fb1\u0f44\u0f0b\u0f0d contains pecha title \\ \\ \u0f54\u0f7c\u0f0b\u0f4f\u0f72\u0f60\u0f72\u0f0b\u0f58\u0f5a\u0f53\u0f0b\u0f56\u0fb1\u0f44\u0f0b\u0f0d contains poti title \\ \\ \u0f63\u0f7a\u0f60\u0f74\u0f0b\u0f61\u0f72\u0f0b\u0f58\u0f5a\u0f53\u0f0b\u0f56\u0fb1\u0f44\u0f0b\u0f0d contains chapter title \\ \\"},{"location":"data/hfml/#notes","title":"Notes","text":"<ul> <li>we don't encode lines as annotations and generate them on the fly from the pagination layer and line returns in the base</li> </ul>"},{"location":"data/hfml/#sources","title":"Sources","text":"<ul> <li>https://ubsicap.github.io/usfm/</li> <li>[gandhari.org]\\&lt;https://gandhari.org/a_dpreface.php&gt;</li> <li>[esukhia/derge-kangyur]\\&lt;https://github.com/Esukhia/derge-kangyur&gt;</li> <li>https://www.markdownguide.org/extended-syntax/#fn:bignote</li> </ul>"},{"location":"data/intro/","title":"Intro","text":"<p>OpenPecha Data is a collection of 14,000 repositories\u2014and growing\u2014 that each contain free open-source Tibetan text files in the OpenPecha format (OPF), and in some cases aligned translations. </p> <p>Most repos contain individual texts, and some contain collections. These collections include corpuses, such those created to train translation models, and collections of texts, such as various editions of the Kangyur and Tengyur.</p> <p>Developers use OpenPecha Data make corpuses, train large language models, and create Tibetan AI. Publishers use it to create e-texts. Academics use it for data-driven research.</p> <ul> <li> <p> Download a featured dataset</p> <p>Get the latest OpenPecha datasets to train Tibetan-language AI models.</p> <p> Featured datasets</p> </li> <li> <p> Get to know the OPF Format</p> <p>Learn about how the OpenPecha Format is structured and how it works.</p> <p> The OPF Format</p> </li> <li> <p> Understand OpenPecha Data on GitHub</p> <p>Get up to speed on how OpenPecha Data is organized on GitHub.</p> <p> OpenPecha Data on GitHub</p> </li> </ul>"},{"location":"data/notes-on-tibetan-language-tech/","title":"Notes on Tibetan Language Tech","text":"<p>Preserving all these old texts might have seemed counterintuitive for a project aiming at revitalizing the publishing and freelancing industry. But in a century powered by data where knowledge and information becomes a prized commodity, BDRC\u2019s effort in preserving and digitizing all this data starts to make a LOT of sense. The project is moving from a past-focused preservation project to a forward looking data project providing the building blocks for the creation of tomorrow\u2019s culture. </p> <p>The Google OCR (optical character recognition) project marks the start of open big data for Tibetan.</p> <p>Currently, the Tibetan language lacks three building blocks to use the language tools available to users of languages like English and Chinese.</p> <ol> <li>A machine-readable version of the language\u2019s complete literature</li> <li>Standards and tools to define and segment words and sentences</li> <li>Standards and tools to define parts of speech and other linguistic annotations</li> </ol> <p>All forms of language-based artificial intelligence from machine translation and search engines to spell checkers and sentiment analysis in advertising depend on the first building block (huge amounts of data). Most AI models need to be trained with sentences and some also need to be trained with words and therefore rely on the second building block (defining words and sentences). Virtually all publication and education technology depend on all three building blocks.</p> <p>The current Google OCR project of creating a machine-encoded text version of BDRC\u2019s collection goes a long way toward creating the first building block. These sets will be available for anyone to use in the form of raw data. This will be most useful for large corporations (FAANG). We will also train some models and make some pre-trained models available. This will be useful for small and medium sized companies, as well as freelancers who have the ability to fine-tune the models.</p> <p>the first of three steps to prepare the data sets to </p> <p>Foundation models that can be fine-tuned for specific applications or domains</p> <ol> <li>FAANG and the like: Out of 15 major AI applications (machine translation, speech-to-text, search engines, etc.) only these two have been developed:<ol> <li>Google OCR is available to use for profit</li> <li>Microsoft Bing Translate has a Machine Translation foundation model that can be fine-tuned for profit</li> </ol> </li> <li>Freelancers can make money by fine-tune these large models (foundation models) and selling them for specific purposes (already possible with Bing Translate)</li> <li>Educators and researchers can use </li> <li>Large datasets<ol> <li>Raw text datasets on OpenPecha</li> <li>AI modelsw</li> </ol> </li> <li>Word and sentence segmentation standards and tools<ol> <li>Segmented datasets</li> <li>Open and free tools like pybo and botok</li> <li>AI models</li> </ol> </li> <li>Linguistic annotation standards and tools<ol> <li>Annotated datasets</li> <li>Annotation standard like Universal POS tags</li> <li>Industry standard tools like spaCy</li> <li>Datasets on tools like SketchEngine</li> <li>AI models</li> </ol> </li> </ol> <p>1. Turning images into machine readable text (image to make text machine-readable e-texts) is the first step in democratizing Tibet. It will be followed by 2) defining words and sentences. Tibetan is a \u201ccontinuous script\u201d, with no a need a standard way to segment cut text into words. Tibetan still doesn\u2019t have a consensus on what a word and sentence is. This has hug impact on technology. Tib can\u2019t use a lot of tools that other languages use because of this: spell checking, analyzing sentiment, etc. All depends on these two steps. 3) Linguistic annotations</p> <p>These three steps bring Tibetan up to speed to use tools available to other languages</p> <p>Impact on two levels </p> <ol> <li> <p>Outside of China big companies, FAANG and in China Waidoo, Alibaba, etc. these companies train their translation models. When these data sets are made available, these companies will use the data and improve it. Got in touch with several including google and microsoft. They said that if we have prepared data, they will incorporate it into tools. Wide reaching impact. </p> <p>These allow search engines to analyze text to provide better search. They can\u2019t search for words in Tibetan. Because of the lack of consensus about what a word is. Digital divide between English or Chinese native speakers and Tibetans is huge. Searching on Google doesn\u2019t work for Tibetan. So they are not yet in the 21st century. Jobs, knowledge, and education is impaired because of this. A big lama said that Tibetans think that they have much less opportunity for grants, jobs, etc. But that isn\u2019t true, they just aren\u2019t good at looking for information. This gives them a lack of self-confidence.</p> <p>writing. English has auto suggestions, and grammar checkers. Without this, Tibetans are shy about writing. They think that they will make a lot of mistakes. They compare themselves to English or Chinese writers and feel a lack of confidence.</p> <p>Teaching. Books available at appropriate reading level. Challenging but not too challenging. By grade level Lexile (book recommendation by grade level, Scholastic. Done with AI. </p> <p>All possible by having access to large data sets. Not possible now because Tibetan doesn\u2019t have this. Often Tibetan children get assigned books that are too hard. Then they lose confidence and don\u2019t write much. Their reading and writing competence is lower than speakers of other languages. Unless China tackles this, it won\u2019t happen. But in small countries without much money, these tools have helped them. </p> </li> <li> <p>Future proof in the next 10 years major companies make models and then smaller companies can </p> </li> <li> <p>Voice-controlled assistants like Siri and Alexa.</p> </li> <li>Natural language generation for question answering by customer service chatbots.</li> <li>Streamlining the recruiting process on sites like LinkedIn by scanning through people\u2019s listed skills and experience.</li> <li>Tools like Grammarly which use NLP to help correct errors and make suggestions for simplifying complex writing.</li> <li>Language models like autocomplete which are trained to predict the next words in a text, based on what has already been typed.</li> <li>Google Neural Machine Translation (GNMT)</li> <li>NLP is widely used in healthcare. It is particularly useful in aggregating information from electronic health record systems, which is full of unstructured data. </li> <li>It can help with all kinds of NLP tasks like tokenising (also known as word segmentation), part-of-speech tagging, creating text classification datasets, and much more.</li> <li>Syntax analysis</li> <li>This is often linked to sentiment analysis</li> <li>lemmatization and stemming.</li> <li>Summarisation is an NLP task that is often used in journalism and on the many newspaper sites that need to summarize news stories. </li> <li>Named entity recognition (NER) is also used on these sites to help with tagging and displaying related stories in a hierarchical order on the web page.</li> </ol>"},{"location":"data/opf-format/","title":"OpenPecha format","text":"<p>The OpenPecha Format (OPF) is a standoff markdown file format in which annotation layers are linked to a base text layer. Virtually unlimited annotaton layers can be added for layers of same-type tags, witnesses, commentaries, translations, and more.</p> <p>OpenPecha's Character Coordinate Translation Vector (CCTV) ties tags in annotation layers to characters in the base layer. This means that whenever a character in the base layer changes position, annotations that link to it are automatically updated to point to its new coordinates.</p>"},{"location":"data/opf-format/#on-this-page","title":"On this page","text":"<ul> <li> File structure</li> <li> Layers</li> <li> Repos and versioning</li> </ul>"},{"location":"data/opf-format/#opf-file-structure","title":"OPF file structure","text":"<p>OPF is an open folder format, which means it\u2019s not a compiled file, but simply an open folder with a specific hierarchy. An OpenPecha folder contains the following folders and files:</p> <ul> <li>A root directory with unique and persistent OpenPecha ID, e.g. P000780 .</li> <li>An <code>index.yml</code> file that contains an index or table of contents for the text.</li> <li>A <code>meta.yml</code> file that contains metadata about the file</li> <li>A base folder that contains one <code>.txt</code> file for each volume in the work</li> <li>A layers folders that contain a <code>.yml</code> file for each annotation layer. These are linked to the base layer text. For example, title is a layer with formatting annotations similar to the  inline tag in HTML.</li> </ul> <p>A sample OPF file might have an internal structure like this:</p> <ul> <li>\ud83d\udcc1  P000780.opf<ul> <li>\ud83d\udcc4 index.yml</li> <li>\ud83d\udcc4 meta.yml</li> <li>\ud83d\udcc1 base<ul> <li>v001.txt</li> <li>v002.txt</li> </ul> </li> <li>\ud83d\udcc1 layers<ul> <li>\ud83d\udcc1 v001<ul> <li>\ud83d\udcc4 Title.yml</li> <li>\ud83d\udcc4 Author.yml</li> <li>\ud83d\udcc4 Tsawa.yml</li> <li>\ud83d\udcc4 Yigchung.yml</li> </ul> </li> <li>\ud83d\udcc1 v002<ul> <li>\ud83d\udcc4 title.yml</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Here is a live example P0000001</p>"},{"location":"data/opf-format/#layers","title":"Layers","text":"<p>Each layer is represented by a <code>.yml</code> file and contains a collection of particular types of annotations with shared attributes.</p>"},{"location":"data/opf-format/#types-of-layers","title":"Types of layers","text":"<pre><code>class LayerEnum(Enum):\n    index = \"index\"\n\n    book_title = \"BookTitle\"\n    sub_title = \"SubTitle\"\n    book_number = \"BookNumber\"\n    poti_title = \"PotiTitle\"\n    author = \"Author\"\n    chapter = \"Chapter\"\n\n    topic = \"Text\"\n    sub_topic = \"SubText\"\n\n    pagination = \"Pagination\"\n    citation = \"Citation\"\n    correction = \"Correction\"\n    error_candidate = \"ErrorCandidate\"\n    peydurma = \"Peydurma\"\n    sabche = \"Sabche\"\n    tsawa = \"Tsawa\"\n    yigchung = \"Yigchung\"\n    archaic = \"Archaic\"\n    durchen = \"Durchen\"\n    footnote = \"Footnote\"\n    segment = \"Segment\"\n</code></pre>"},{"location":"data/opf-format/#index-layer","title":"Index layer","text":"<p>The index layer is similar to a table of contents and contains text and subtext annotations. The index file splits a text into subsections, and assigns to these sections unique identifiers (UUIDs). Annotation references are also stored in the index as a unique ID associated with a span of characters. Whenever there\u2019s a change to the base text, these spans are updated. When an annotation is referred to outside the index, however, it isn\u2019t referred to as a span (as in a tag system like XML), but as an ID.</p> <p>Example:</p> <pre><code>id: 68f9113d7a7f4f97b1c61af77251e6d7\nannotation_type: index\nrevision: '00001'\nannotations:\n51f58796058b461ab32f3c972ee5417c:\nwork_id: T1\nparts:\n3cbe647abf404688a79c24d14742826c:\nwork_id: T1-1\nspan:\n- vol: 1\nstart: 27\nend: 396711\nspan:\n- vol: 1\nstart: 27\nend: 934579\n</code></pre>"},{"location":"data/opf-format/#example-layer","title":"Example layer","text":"<p>Here's the example of a correction layer:</p> <pre><code>id: 2ea1861be051406a858307cd592ef5ec\nannotation_type: Correction\nrevision: '00001'\nannotations:\n1e19a11e32d54d7897021d5be594d563:\ncorrection: \u0f58\u0f46\u0f72\u0f60\u0f7c\u0f0b\ncertainty: null\nspan:\nstart: 145863\nend: 145868\n497e4044c77b4877a233a3c98b267672:\ncorrection: \u0f58\u0f46\u0f72\u0f60\u0f7c\u0f0b\ncertainty: null\nspan:\nstart: 145966\nend: 145971\n</code></pre>"},{"location":"data/opf-format/#openpecha-data-repositories-and-versioning","title":"OpenPecha Data repositories and versioning","text":"<p>OpenPecha etexts are stored as Git repositories that include the following branches:</p> <ul> <li>Master<ul> <li>The master branch contains a <code>.opf</code> folder, which is protected</li> <li>Only admins and repo owners can directly update the master branch from the other online branches</li> </ul> </li> <li>Publication<ul> <li>The publication branch is for collaboratively improving the text</li> <li>Only admin or text owners can merge changes from the publication branch to the master branch</li> <li>GitHub Actions is set up to update the <code>.opf</code> folder in the master branch when the publication branch changes</li> </ul> </li> <li>Custom repos<ul> <li>Here users outside of the collaboration team can edit texts and export them in various formats</li> <li>These edits are not recorded and don't update the master branch OPF files</li> <li>The exported text is released in the temp section of the release</li> </ul> </li> <li>Releases<ul> <li>The initial section contains the src text as-is in its original format, e.g. <code>.txt</code>, <code>.epub</code>, <code>.hfml</code>, <code>.docx</code>, etc.</li> <li>The temp section contains a user's exported text outside of the collaboration</li> <li>The V### section contains the official release of the exported text</li> </ul> </li> </ul>"},{"location":"data/reference/","title":"Data reference","text":""},{"location":"data/toolkit-api/","title":"Toolkit API","text":"<p>All the annotations in OPF are just span of start and end character the base layer and some attributes of the particular annotation. The most simplest form of annotation is just having the a span. Here is the list of annotation currently supported by OpenPecha.</p> <p>Annotations in OpenPecha are broadly categorized into Physical and Semantic annotations.</p>"},{"location":"data/toolkit-api/#semantic-annotations","title":"Semantic Annotations","text":"<p>Any annotations from the Verbal text</p>"},{"location":"data/toolkit-api/#citation","title":"Citation","text":"<p>Citation</p> JSON Schema <pre><code>{\n\"title\": \"Citation\",\n\"type\": \"object\",\n\"properties\": {\n\"id\": {\n\"title\": \"Id\",\n\"type\": \"string\"\n},\n\"span\": {\n\"$ref\": \"#/definitions/Span\"\n},\n\"metadata\": {\n\"title\": \"Metadata\",\n\"default\": {},\n\"type\": \"object\"\n}\n},\n\"required\": [\n\"span\"\n],\n\"additionalProperties\": false,\n\"definitions\": {\n\"Span\": {\n\"title\": \"Span\",\n\"type\": \"object\",\n\"properties\": {\n\"start\": {\n\"title\": \"Start\",\n\"minimum\": 0,\n\"type\": \"integer\"\n},\n\"end\": {\n\"title\": \"End\",\n\"minimum\": 0,\n\"type\": \"integer\"\n}\n},\n\"required\": [\n\"start\",\n\"end\"\n],\n\"additionalProperties\": false\n}\n}\n}\n</code></pre> Python example <pre><code>from openpecha.core.annotations import Citation, Span\n\ncitation = Citation(span=Span(start=10, end=30))\n\nassert citation.span.start == 10\n</code></pre>"},{"location":"data/toolkit-api/#correction","title":"Correction","text":"<p>Correction</p> JSON Schema <pre><code>{\n\"title\": \"Correction\",\n\"type\": \"object\",\n\"properties\": {\n\"id\": {\n\"title\": \"Id\",\n\"type\": \"string\"\n},\n\"span\": {\n\"$ref\": \"#/definitions/Span\"\n},\n\"metadata\": {\n\"title\": \"Metadata\",\n\"default\": {},\n\"type\": \"object\"\n}\n},\n\"required\": [\n\"span\"\n],\n\"additionalProperties\": false,\n\"definitions\": {\n\"Span\": {\n\"title\": \"Span\",\n\"type\": \"object\",\n\"properties\": {\n\"start\": {\n\"title\": \"Start\",\n\"minimum\": 0,\n\"type\": \"integer\"\n},\n\"end\": {\n\"title\": \"End\",\n\"minimum\": 0,\n\"type\": \"integer\"\n}\n},\n\"required\": [\n\"start\",\n\"end\"\n],\n\"additionalProperties\": false\n}\n}\n}\n</code></pre> Python example <pre><code>from openpecha.core.annotations import Correction, Span\n\ncorrection = Correction(span=Span(start=10, end=30))\n\nassert correction.span.start == 10\n</code></pre>"},{"location":"data/toolkit-api/#errorcandidate","title":"ErrorCandidate","text":"<p>ErrorCandidate</p> JSON Schema <pre><code>{\n\"title\": \"ErrorCandidate\",\n\"type\": \"object\",\n\"properties\": {\n\"id\": {\n\"title\": \"Id\",\n\"type\": \"string\"\n},\n\"span\": {\n\"$ref\": \"#/definitions/Span\"\n},\n\"metadata\": {\n\"title\": \"Metadata\",\n\"default\": {},\n\"type\": \"object\"\n}\n},\n\"required\": [\n\"span\"\n],\n\"additionalProperties\": false,\n\"definitions\": {\n\"Span\": {\n\"title\": \"Span\",\n\"type\": \"object\",\n\"properties\": {\n\"start\": {\n\"title\": \"Start\",\n\"minimum\": 0,\n\"type\": \"integer\"\n},\n\"end\": {\n\"title\": \"End\",\n\"minimum\": 0,\n\"type\": \"integer\"\n}\n},\n\"required\": [\n\"start\",\n\"end\"\n],\n\"additionalProperties\": false\n}\n}\n}\n</code></pre> Python example <pre><code>from openpecha.core.annotations import ErrorCandidate, Span\n\nerror_candidate = ErrorCandidate(span=Span(start=10, end=30))\n\nassert error_candidate.span.start == 10\n</code></pre>"},{"location":"data/toolkit-api/#pedurma","title":"Pedurma","text":"<p>Pedurma</p> JSON Schema: <pre><code>{\n\"title\": \"Pedurma\",\n\"type\": \"object\",\n\"properties\": {\n\"id\": {\n\"title\": \"Id\",\n\"type\": \"string\"\n},\n\"span\": {\n\"$ref\": \"#/definitions/Span\"\n},\n\"metadata\": {\n\"title\": \"Metadata\",\n\"default\": {},\n\"type\": \"object\"\n}\n},\n\"required\": [\n\"span\"\n],\n\"additionalProperties\": false,\n\"definitions\": {\n\"Span\": {\n\"title\": \"Span\",\n\"type\": \"object\",\n\"properties\": {\n\"start\": {\n\"title\": \"Start\",\n\"minimum\": 0,\n\"type\": \"integer\"\n},\n\"end\": {\n\"title\": \"End\",\n\"minimum\": 0,\n\"type\": \"integer\"\n}\n},\n\"required\": [\n\"start\",\n\"end\"\n],\n\"additionalProperties\": false\n}\n}\n}\n</code></pre> Python example: <pre><code>from openpecha.core.annotations import Pedurma, Span\n\npedurma = Pedurma(span=Span(start=10, end=30))\n\nassert pedurma.span.start == 10\n</code></pre>"},{"location":"data/toolkit-api/#sabche","title":"Sabche","text":"<p>Sabche</p> JSON Schema <pre><code>{\n\"title\": \"Sabche\",\n\"type\": \"object\",\n\"properties\": {\n\"id\": {\n\"title\": \"Id\",\n\"type\": \"string\"\n},\n\"span\": {\n\"$ref\": \"#/definitions/Span\"\n},\n\"metadata\": {\n\"title\": \"Metadata\",\n\"default\": {},\n\"type\": \"object\"\n}\n},\n\"required\": [\n\"span\"\n],\n\"additionalProperties\": false,\n\"definitions\": {\n\"Span\": {\n\"title\": \"Span\",\n\"type\": \"object\",\n\"properties\": {\n\"start\": {\n\"title\": \"Start\",\n\"minimum\": 0,\n\"type\": \"integer\"\n},\n\"end\": {\n\"title\": \"End\",\n\"minimum\": 0,\n\"type\": \"integer\"\n}\n},\n\"required\": [\n\"start\",\n\"end\"\n],\n\"additionalProperties\": false\n}\n}\n}\n</code></pre> Python example <pre><code>from openpecha.core.annotations import Sabche, Span\n\nsabche = Sabche(span=Span(start=10, end=30))\n\nassert sabche.span.start == 10\n</code></pre>"},{"location":"data/toolkit-api/#tsawa","title":"Tsawa","text":"<p>Tsawa</p> JSON Schema <pre><code>{\n\"title\": \"Tsawa\",\n\"type\": \"object\",\n\"properties\": {\n\"id\": {\n\"title\": \"Id\",\n\"type\": \"string\"\n},\n\"span\": {\n\"$ref\": \"#/definitions/Span\"\n},\n\"metadata\": {\n\"title\": \"Metadata\",\n\"default\": {},\n\"type\": \"object\"\n}\n},\n\"required\": [\n\"span\"\n],\n\"additionalProperties\": false,\n\"definitions\": {\n\"Span\": {\n\"title\": \"Span\",\n\"type\": \"object\",\n\"properties\": {\n\"start\": {\n\"title\": \"Start\",\n\"minimum\": 0,\n\"type\": \"integer\"\n},\n\"end\": {\n\"title\": \"End\",\n\"minimum\": 0,\n\"type\": \"integer\"\n}\n},\n\"required\": [\n\"start\",\n\"end\"\n],\n\"additionalProperties\": false\n}\n}\n}\n</code></pre> Python example <pre><code>from openpecha.core.annotations import Span, Tsawa\n\ntsawa = Tsawa(span=Span(start=10, end=30))\n\nassert tsawa.span.start == 10\n</code></pre>"},{"location":"data/toolkit-api/#yigchung","title":"Yigchung","text":"<p>Yigchung</p> JSON Schema <pre><code>{\n\"title\": \"Yigchung\",\n\"type\": \"object\",\n\"properties\": {\n\"id\": {\n\"title\": \"Id\",\n\"type\": \"string\"\n},\n\"span\": {\n\"$ref\": \"#/definitions/Span\"\n},\n\"metadata\": {\n\"title\": \"Metadata\",\n\"default\": {},\n\"type\": \"object\"\n}\n},\n\"required\": [\n\"span\"\n],\n\"additionalProperties\": false,\n\"definitions\": {\n\"Span\": {\n\"title\": \"Span\",\n\"type\": \"object\",\n\"properties\": {\n\"start\": {\n\"title\": \"Start\",\n\"minimum\": 0,\n\"type\": \"integer\"\n},\n\"end\": {\n\"title\": \"End\",\n\"minimum\": 0,\n\"type\": \"integer\"\n}\n},\n\"required\": [\n\"start\",\n\"end\"\n],\n\"additionalProperties\": false\n}\n}\n}\n</code></pre> Python example <pre><code>from openpecha.core.annotations import Span, Yigchung\n\nyigchung = Yigchung(span=Span(start=10, end=30))\n\nassert yigchung.span.start == 10\n</code></pre>"},{"location":"data/toolkit-api/#archaic","title":"Archaic","text":"<p>Archaic</p> JSON Schema <pre><code>{\n\"title\": \"Archaic\",\n\"type\": \"object\",\n\"properties\": {\n\"id\": {\n\"title\": \"Id\",\n\"type\": \"string\"\n},\n\"span\": {\n\"$ref\": \"#/definitions/Span\"\n},\n\"metadata\": {\n\"title\": \"Metadata\",\n\"default\": {},\n\"type\": \"object\"\n}\n},\n\"required\": [\n\"span\"\n],\n\"additionalProperties\": false,\n\"definitions\": {\n\"Span\": {\n\"title\": \"Span\",\n\"type\": \"object\",\n\"properties\": {\n\"start\": {\n\"title\": \"Start\",\n\"minimum\": 0,\n\"type\": \"integer\"\n},\n\"end\": {\n\"title\": \"End\",\n\"minimum\": 0,\n\"type\": \"integer\"\n}\n},\n\"required\": [\n\"start\",\n\"end\"\n],\n\"additionalProperties\": false\n}\n}\n}\n</code></pre> Python example <pre><code>from openpecha.core.annotations import Archaic, Span\n\narchaic = Archaic(span=Span(start=10, end=30))\n\nassert archaic.span.start == 10\n</code></pre>"},{"location":"data/toolkit-api/#durchen","title":"Durchen","text":"<p>Durchen</p> JSON Schema <pre><code>{\n\"title\": \"Durchen\",\n\"type\": \"object\",\n\"properties\": {\n\"id\": {\n\"title\": \"Id\",\n\"type\": \"string\"\n},\n\"span\": {\n\"$ref\": \"#/definitions/Span\"\n},\n\"metadata\": {\n\"title\": \"Metadata\",\n\"default\": {},\n\"type\": \"object\"\n},\n\"default\": {\n\"title\": \"Default\",\n\"description\": \"text_name of the default option\",\n\"type\": \"string\"\n},\n\"options\": {\n\"title\": \"Options\",\n\"description\": \"all other spell options in dict of {text_name, option}\",\n\"type\": \"object\",\n\"additionalProperties\": {\n\"type\": \"string\"\n}\n}\n},\n\"required\": [\n\"span\",\n\"default\",\n\"options\"\n],\n\"additionalProperties\": false,\n\"definitions\": {\n\"Span\": {\n\"title\": \"Span\",\n\"type\": \"object\",\n\"properties\": {\n\"start\": {\n\"title\": \"Start\",\n\"minimum\": 0,\n\"type\": \"integer\"\n},\n\"end\": {\n\"title\": \"End\",\n\"minimum\": 0,\n\"type\": \"integer\"\n}\n},\n\"required\": [\n\"start\",\n\"end\"\n],\n\"additionalProperties\": false\n}\n}\n}\n</code></pre> Python example <pre><code>from openpecha.core.annotations import Durchen, Span\n\nbase = \"\u0f66\u0f7a\u0f58\u0f66\u0f0b\u0f45\u0f53\u0f0b\u0f62\u0fa3\u0f58\u0f66\u0f0b\u0f63\u0f0b\u0f66\u0fb3\u0f74\u0f0b\u0f56\u0f0b\u0f61\u0f72\u0f0d\"\ndurchen = Durchen(\n    span=Span(start=15, end=18),\n    default=\"\u0f66\u0fa1\u0f7a\u0f0d\",\n    options={\"\u0f45\u0f7c\u0f0d\": \"\u0f66\u0fb3\u0f74\u0f0b\", \"\u0f54\u0f7a\u0f0d\": \"\u0f56\u0f66\u0fb3\u0f74\u0f0d\", \"\u0f66\u0fa3\u0f62\u0f0d\": \"\u0f56\u0f66\u0fb3\u0f74\u0f0d\"},\n)\n\nassert base[durchen.span.start : durchen.span.end + 1] == \"\u0f66\u0fb3\u0f74\u0f0b\"\n</code></pre>"},{"location":"data/toolkit-api/#footnote","title":"Footnote","text":"<p>Footnote</p> JSON Schema <pre><code>{\n\"title\": \"Footnote\",\n\"type\": \"object\",\n\"properties\": {\n\"id\": {\n\"title\": \"Id\",\n\"type\": \"string\"\n},\n\"span\": {\n\"$ref\": \"#/definitions/Span\"\n},\n\"metadata\": {\n\"title\": \"Metadata\",\n\"default\": {},\n\"type\": \"object\"\n}\n},\n\"required\": [\n\"span\"\n],\n\"additionalProperties\": false,\n\"definitions\": {\n\"Span\": {\n\"title\": \"Span\",\n\"type\": \"object\",\n\"properties\": {\n\"start\": {\n\"title\": \"Start\",\n\"minimum\": 0,\n\"type\": \"integer\"\n},\n\"end\": {\n\"title\": \"End\",\n\"minimum\": 0,\n\"type\": \"integer\"\n}\n},\n\"required\": [\n\"start\",\n\"end\"\n],\n\"additionalProperties\": false\n}\n}\n}\n</code></pre> Python example <pre><code>from openpecha.core.annotations import Footnote, Span\n\nfootnote = Footnote(span=Span(start=10, end=30))\n\nassert footnote.span.start == 10\n</code></pre>"},{"location":"data/toolkit-api/#segment","title":"Segment","text":"<p>Segment of an alignment</p> JSON Schema <pre><code>{\n\"title\": \"Segment\",\n\"type\": \"object\",\n\"properties\": {\n\"id\": {\n\"title\": \"Id\",\n\"type\": \"string\"\n},\n\"span\": {\n\"$ref\": \"#/definitions/Span\"\n},\n\"metadata\": {\n\"title\": \"Metadata\",\n\"default\": {},\n\"type\": \"object\"\n}\n},\n\"required\": [\n\"span\"\n],\n\"additionalProperties\": false,\n\"definitions\": {\n\"Span\": {\n\"title\": \"Span\",\n\"type\": \"object\",\n\"properties\": {\n\"start\": {\n\"title\": \"Start\",\n\"minimum\": 0,\n\"type\": \"integer\"\n},\n\"end\": {\n\"title\": \"End\",\n\"minimum\": 0,\n\"type\": \"integer\"\n}\n},\n\"required\": [\n\"start\",\n\"end\"\n],\n\"additionalProperties\": false\n}\n}\n}\n</code></pre> Python example <pre><code>from openpecha.core.annotations import Segment, Span\n\nsegment = Segment(span=Span(start=10, end=30))\n\nassert segment.span.start == 10\n</code></pre>"},{"location":"data/toolkit-api/#physical-annotations","title":"Physical Annotations","text":""},{"location":"data/toolkit-api/#booktitle","title":"BookTitle","text":"<p>Title of the book</p>"},{"location":"data/toolkit-api/#subtitle","title":"SubTitle","text":"<p>Sub title of the book</p>"},{"location":"data/toolkit-api/#edition","title":"Edition","text":"<p>It can be Edition number/name. Previously called BookNumber.</p>"},{"location":"data/toolkit-api/#author","title":"Author","text":"<p>author of the book</p>"},{"location":"data/toolkit-api/#chapter","title":"Chapter","text":"<p>Chapter title</p>"},{"location":"data/toolkit-api/#text","title":"Text","text":"<p>Represents text and used in Index layer.</p>"},{"location":"data/toolkit-api/#subtext","title":"SubText","text":"<p>Represents Sub text and used in Index layer.</p>"},{"location":"data/toolkit-api/#pagination","title":"Pagination","text":"<p>Represents the single page of a text.</p> JSON Schema <pre><code>{\n\"title\": \"Pagination\",\n\"type\": \"object\",\n\"properties\": {\n\"id\": {\n\"title\": \"Id\",\n\"type\": \"string\"\n},\n\"span\": {\n\"$ref\": \"#/definitions/Span\"\n},\n\"metadata\": {\n\"title\": \"Metadata\",\n\"default\": {},\n\"type\": \"object\"\n},\n\"page_info\": {\n\"title\": \"Page Info\",\n\"description\": \"page payload\",\n\"type\": \"string\"\n},\n\"imgnum\": {\n\"title\": \"Imgnum\",\n\"description\": \"image sequence number\",\n\"type\": \"integer\"\n},\n\"order\": {\n\"title\": \"Order\",\n\"description\": \"order of the page\",\n\"type\": \"integer\"\n},\n\"reference\": {\n\"title\": \"Reference\",\n\"description\": \"can be url or just string indentifier of source page\",\n\"type\": \"string\"\n}\n},\n\"required\": [\n\"span\"\n],\n\"additionalProperties\": false,\n\"definitions\": {\n\"Span\": {\n\"title\": \"Span\",\n\"type\": \"object\",\n\"properties\": {\n\"start\": {\n\"title\": \"Start\",\n\"minimum\": 0,\n\"type\": \"integer\"\n},\n\"end\": {\n\"title\": \"End\",\n\"minimum\": 0,\n\"type\": \"integer\"\n}\n},\n\"required\": [\n\"start\",\n\"end\"\n],\n\"additionalProperties\": false\n}\n}\n}\n</code></pre> Python example <pre><code>from openpecha.core.annotations import Pagination, Span\n\npagination = Pagination(span=Span(start=10, end=30))\n\nassert pagination.span.start == 10\n</code></pre>"},{"location":"data/what-is-openpecha/","title":"What is openpecha","text":""},{"location":"data/what-is-openpecha/#what-is-openpecha","title":"What is OpenPecha?","text":"<p>As we\u2019ve touched on above, a truly modern digital publication is never a \u201cfinal\u201d, static image. Instead, it\u2019s a living, breathing entity in constant communication with its community. The Tibetan digital landscape needs an open space where data can be gathered; on offer to anyone in the community; and open to collaborative curation and annotation, while also responding to users\u2019 needs. This kind of public-domain, digital publishing will be familiar to anyone who\u2019s heard of Project Gutenburg and its Distributed Proofreaders platform, and the offshoots they\u2019ve inspired (GITenberg, WikiBooks, etc.). From a practical standpoint, that means we need a tool that imports from, and exports to, formats common to users who access Tibetan texts; that transfers annotations from one base text to another; and that offers an organized, systematic catalog of titles, versions, annotations, and related texts. This data-first maintenance and interoperability are the core aims of OpenPecha\u2019s database, format, and toolkit.</p> <p>First, it aims to store every available electronic representation of each text, along with all existing annotations. From there, both the texts and their annotations are downloadable; contributions are uploadable; and users can export to a variety of common formats (like EPUB and PDF). This means that texts can keep improving while not losing any of their annotations. It also allows for any NLP (Natural Language Processing), NER (Named Entity Recognition), or error-detecting models to be applied to the entirety of the corpus without interfering with existing data created by others. Finally, it gives the community of readers access to the most-current proofread ebooks that contain only the annotation layers they need, in the format of their choice. In short, the goal is to provide something that is:</p> <ul> <li>Flexible in application, serving a wide variety of Tibetan language professionals;</li> <li>Durable in that it uses free tools and platforms, with minimal maintenance;</li> <li>Editable at all stages of the workflow, and at every level of the document;</li> <li>Easy-to-learn and human-friendly for the average user;</li> <li>Open to anyone anywhere, providing a large catalog of texts with a wide range of annotation schema; and</li> <li>Collaborative, to leverage shared knowledge in a small, specialized field, and allow for crowdsourced improvements.</li> </ul>"},{"location":"data/who-is-openpecha-for/","title":"Who is openpecha for","text":""},{"location":"data/who-is-openpecha-for/#who-is-openpecha-for","title":"Who is OpenPecha for?","text":"<ul> <li> <p>Text Owners. Text owners are groups of people who are linked to a particular Tibetan text through their history, culture, and heritage, and have a continuing relationship that brings the text into living, communicative contexts; this includes, for example, living monastic traditions that study the particular texts of their Buddhist lineage or research projects who specialize in a given collection</p> <ul> <li>Find contributors for their own collections</li> <li>Manage pechas in their own collections (admin Master branches)</li> </ul> </li> <li> <p>Academics &amp; Researchers in Tibetan Studies; Philology; Digital Humanities; Linguistics; Monastic Settings; or any other field who need to access, read, translate, edit, or annotate Tibetan texts. OPF allows them to:</p> <ul> <li>Build custom corpuses with any annotations they like</li> <li>Contribute texts or annotations</li> <li>Update texts and annotations</li> <li>Export works in their preferred format</li> </ul> </li> <li> <p>Publishers</p> <ul> <li>Download the best &amp; latest version of a text</li> <li>Download or contribute annotations</li> <li>Export in preferred format (EPUB for publishing, etc.)</li> </ul> </li> <li> <p>Digital Libraries</p> <ul> <li>Connect their user interface to OpenPecha to fetch etexts and the annotations they need (IIIF, International Image Interoperability Framework, for image libraries, etc.)</li> <li>Update and contribute new annotations via Github API</li> <li>Add improvements to the toolkit according to their needs</li> <li>Use OpenPecha as an editing tool for updating their content as part of a dynamic publishing workflow</li> </ul> </li> <li> <p>Readers</p> <ul> <li>Export the latest &amp; best version of a text in their preferred format</li> </ul> </li> <li> <p>Programmers</p> <ul> <li>Link to the best version of texts with annotations through API (Application Programming Interface)</li> </ul> </li> </ul>"},{"location":"data/why-openpecha/","title":"Why openpecha","text":""},{"location":"data/why-openpecha/#why-openpecha","title":"Why OpenPecha?","text":"<p>Digital text is quickly becoming essential to modern daily life. The article you\u2019re reading right now is born digital; unlike texts of the not-so-distant past, it may never be printed at all. Worldwide, the trend is clear: Digital text is on the way in, and print is on its way out. Year-by-year, more and more readers are turning to ebooks, internet news, and other forms of ereading, while generation-by-generation, print is becoming less and less relevant.</p> <p>These trends aren\u2019t unique to English\u2014to meet the demands and expectations of today\u2019s readers, Tibetan texts, too, are being digitized by many organizations and institutions with a shared appreciation for the Tibetan literary heritage. They include a variety of secular publishers, monastic institutions, and Buddhist foundations, among others. But while these organizations share common goals for common texts, their work is all too frequently completely disconnected from the community at large.</p> <p>This situation negatively impacts what is already a minoritized and under-resourced language. While competition\u2014from other languages, as well as other publishers in the Tibetan etext world\u2014has been a driver of innovation in the adoption of ereading technology, we believe that a rich, shared data source is not only in everyone\u2019s best interest, but the only practical way forward when we consider the time, effort, expertise, and money that quality digitization takes.</p> <p>That\u2019s why we\u2019ve designed OpenPecha to be a public, open platform for collaborative etext curation and annotation sharing. Its aim is providing a wide range of users with the latest version of the exact \u201cview\u201d of any text they need, while maintaining the integrity of the text and its annotations, and simultaneously allowing for community improvements and additions. In this paper, we explore the details of how the project came to be; what it is and how it works; while also presenting a few common use cases.</p>"},{"location":"toolkit/acquisition-pipeline/","title":"Acquisition pipeline","text":""},{"location":"toolkit/etext-operations/","title":"E-text operations","text":""},{"location":"toolkit/install/","title":"Installation","text":"<p>Stable version:</p>"},{"location":"toolkit/install/#pip-install-openpecha","title":"<code>pip install openpecha</code>","text":"<p>Daily development version:</p>"},{"location":"toolkit/install/#pip-install-githttpsgithubcomopenpechaopenpecha-toolkit","title":"<code>pip install git+https://github.com/OpenPecha/Openpecha-Toolkit</code>","text":""},{"location":"toolkit/install/#developer-installation","title":"Developer Installation","text":"<pre><code>git clone https://github.com/OpenPecha-dev/openpecha-toolkit.git\ncd openpecha-toolkit\npip install -r requirements-dev.txt\npip install -e .\npre-commit install\n</code></pre>"},{"location":"toolkit/install/#testing","title":"Testing","text":"<pre><code>PYTHONPATH=.:$PYTHONPATH pytest tests\n</code></pre>"},{"location":"toolkit/layer/","title":"Layer","text":""},{"location":"toolkit/layer/#create-new-layer","title":"Create new Layer","text":"<p>use LayerEnum to create types of layer.</p> <pre><code>from openpecha.core.layer import Layer, LayerEnum\n\nlayer = Layer(annotation_type=LayerEnum.citation)\n\nassert layer.annotation_type == LayerEnum.citation\n</code></pre>"},{"location":"toolkit/layer/#add-annotation-to-layer","title":"Add Annotation to Layer","text":"<p>Refer Annotations docs about OpenPecha annotations.</p> <pre><code>from openpecha.core.annotations import Citation, Span\nfrom openpecha.core.layer import Layer, LayerEnum\n\nlayer = Layer(annotation_type=LayerEnum.citation)\nann = Citation(span=Span(start=10, end=20))\n\nlayer.set_annotation(ann)\n</code></pre>"},{"location":"toolkit/layer/#get-annotation-from-layer","title":"Get Annotation from layer","text":"<pre><code>from openpecha.core.annotations import Citation, Span\nfrom openpecha.core.layer import Layer, LayerEnum\n\nlayer = Layer(annotation_type=LayerEnum.citation)\nann = Citation(span=Span(start=10, end=20))\nann_id = layer.set_annotation(ann)\n\nnew_ann = layer.get_annotation(ann_id)\n</code></pre>"},{"location":"toolkit/layer/#remove-annotation-from-layer","title":"Remove annotation from layer","text":"<pre><code>from openpecha.core.annotations import Citation, Span\nfrom openpecha.core.layer import Layer, LayerEnum\n\nlayer = Layer(annotation_type=LayerEnum.citation)\nann = Citation(span=Span(start=10, end=20))\nlayer.add_annotation(ann)\n\nassert ann.id in layer.annotations\n\nlayer.remove_annotation(ann.id)  # new\n\nassert ann.id not in layer.annotations\n</code></pre>"},{"location":"toolkit/layer/#bump-layer-revision-number","title":"Bump layer revision number","text":"<pre><code>from openpecha.core.layer import Layer, LayerEnum\n\nlayer = Layer(annotation_type=LayerEnum.citation)\n\nassert layer.revision == \"00001\"\n\nlayer.bump_revision()\n\nassert layer.revision == \"00002\"\n</code></pre>"},{"location":"toolkit/metadata/","title":"Pecha Metadata","text":""},{"location":"toolkit/metadata/#create-metadata-for-a-pecha","title":"Create Metadata for a Pecha","text":"<p>Since, OpenPecha as has three types of pecha, we have metadata classes to create metadata for each type of pecha. Here is the list of pecha types with it's associated metadata class.</p> Pecha Type ID format Metadata Class Note Work W######## <code>openpecha.core.metadata.WorkMetadata</code> Initial I######## <code>openpecha.core.metadata.InitialPechaMetadata</code> Diplomatic D######## <code>openpecha.core.metadata.DiplomaticPechaMetadata</code> Open O######## <code>openpecha.core.metadata.OpenPechaMetadata</code> Alignment A######## `` Collection C######## `` <p>here is an example to create metadata for Initial Pecha type</p> <pre><code>from openpecha.core.metadata import InitialCreationType, InitialPechaMetadata\n\nmetadata = InitialPechaMetadata(\n    source=\"https://library.bdrc.io\",\n    source_file=\"https://library.bdrc.io/text.json\",\n    initial_creation_type=InitialCreationType.ocr,\n    parser=\"https://github.com/OpenPecha-dev/openpecha-toolkit/pgoogle_orc.py\",\n    source_metadata={\n        \"id\": \"bdr:W1PD90121\",\n        \"title\": \"\u0f58\u0f60\u0f7c\u0f0b\u0f62\u0fab\u0f7c\u0f44\u0f0b\u0f42\u0f72\u0f0b\u0f66\u0f0b\u0f46\u0f60\u0f72\u0f0b\u0f58\u0f72\u0f44\u0f0b\u0f56\u0f4f\u0f74\u0f66\u0f0d\",\n        \"author\": \"author name\",\n    },\n)\n\n\nassert metadata.id.startswith(\"I\")\n</code></pre> <p>Attention</p> <p>Only the <code>initial_creation_type</code> attribute is required, rest of the attributes are optional.</p> <p>Attention</p> <p>No need to assign pecha id when creating metadata, the metadata class will automatically create id with correct id prefix for a particular pecha type.</p>"},{"location":"toolkit/metadata/#examples","title":"Examples","text":"<p>all developers should follow schema for <code>source_metadata</code> and <code>base</code> as in given examples</p>"},{"location":"toolkit/metadata/#initial-pecha-metadata","title":"Initial Pecha Metadata","text":"<pre><code>id: I7E1A43F2\nsource: https://library.bdrc.io\nsource_file: null\ninitial_creation_type: ocr\nimported: '2020-03-28T12:12:38+00:00'\nlast_modified: '2022-06-08T11:28:52.590761+00:00'\nparser: https://github.com/OpenPecha-dev/openpecha-toolkit/blob/231bba39dd1ba393320de82d4d08a604aabe80fc/openpecha/formatters/google_orc.py\nocr_word_median_confidence_index: 0.9\nsource_metadata:\nid: bdr:W3CN4314\ntitle: \u0f5a\u0f51\u0f0b\u0f58\u0f0b\u0f62\u0f72\u0f42\u0f66\u0f0b\u0f54\u0f60\u0f72\u0f0b\u0f42\u0f4f\u0f7a\u0f62\u0f0b\u0f42\u0fb1\u0f72\u0f0b\u0f62\u0fa9\u0f0b\u0f56\u0f0d\nauthor: ''\naccess: http://purl.bdrc.io/admindata/AccessOpen\nrestrictedInChina: false\nbase:\n529C:\nsource_metadata:\nimage_group_id: I3CN8548\ntitle: ''\ntotal_pages: 62\norder: 1\nbase_file: 529C.txt\nocr_word_median_confidence_index: 0.9\n</code></pre>"},{"location":"toolkit/metadata/#adding-copyright-and-license","title":"Adding Copyright and License","text":"<p>here is an example to add copyright and license in pecha's metadata</p> <pre><code>from openpecha.core.metadata import (\n    Copyright,\n    CopyrightStatus,\n    InitialCreationType,\n    LicenseType,\n    PechaMetadata,\n)\n\ncopyright = Copyright(\n    status=CopyrightStatus.COPYRIGHTED,\n    notice=\"Copyright 2022 OpenPecha\",\n    info_url=\"https://dev.openpecha.org/terms-and-conditions\",\n)\n\nmetadata = PechaMetadata(\n    initial_creation_type=InitialCreationType.input,\n    copyright=copyright,\n    license=LicenseType.CC_BY_NC_SA,\n)\n\nprint(metadata)\n</code></pre>"},{"location":"toolkit/metadata/#copyright-status","title":"Copyright Status","text":"<p>OpenPecha provides three Copyright Status:</p> <ul> <li><code>CopyrightStatus.UNKNOWN</code>, use if the Copyright of the pecha is unknown.</li> <li><code>CopyrightStatus.COPYRIGHTED</code>, use if the pecha source is Copyright restricted.</li> <li><code>CopyRightStatus.PUBLIC_DOMAIN</code>, use if the pecha source is in Public Domain.</li> </ul>"},{"location":"toolkit/metadata/#licenses","title":"Licenses","text":"<p>OpenPecha relies on Creative Common Licenses for licensing any pecha on OpenPecha Repository,</p> <p>We can access Creative Common Licenses through <code>LicenseType</code> enum.</p>"},{"location":"toolkit/pecha/","title":"Pecha","text":""},{"location":"toolkit/pecha/#create-new-pecha","title":"Create new pecha","text":"<pre><code>from openpecha.core.annotations import Citation, Span\nfrom openpecha.core.layer import Layer, LayerEnum\nfrom openpecha.core.metadata import InitialCreationType, InitialPechaMetadata\nfrom openpecha.core.pecha import OpenPechaFS\n\n# create new pecha\nmetadata = InitialPechaMetadata(initial_creation_type=InitialCreationType.input)\npecha = OpenPechaFS(path=\"&lt;path_to_pecha&gt;\", metadata=metadata)\n\n# create a simple layer\nann = Citation(span=Span(start=10, end=20))\nlayer = Layer(annotation_type=LayerEnum.citation)\nlayer.set_annotation(ann)\n\nbase_name = pecha.set_base(\"base content\")\npecha.set_layer(base_name, layer)\n\n# pecha.save()\n</code></pre> <p>When we are creating a brand new pecha, first we must create an instance of <code>PechaMetadata</code>, which will automatically issue a unique id for the pecha and allow us to specify other metadata about the pecha.</p> <p>Then, we will use this metadata object to create pecha container, instance of <code>OpenPechaFS</code>.</p> <p>After adding base and layer(s) we need to call <code>pecha.save()</code> to save pecha to file system in OpenPecha Format.</p>"},{"location":"toolkit/pecha/#adding-base-and-layer","title":"Adding Base and Layer","text":"<p>Refer layer docs to how to create layer.</p> <pre><code>from openpecha.core.annotations import Citation, Span\nfrom openpecha.core.layer import Layer, LayerEnum\nfrom openpecha.core.pecha import OpenPechaFS\n\npecha = OpenPechaFS(path=\"&lt;path_to_pecha&gt;\")\n\n# create a simple layer\nann = Citation(span=Span(start=10, end=20))\nlayer = Layer(annotation_type=LayerEnum.citation)\nlayer.set_annotation(ann)\n\nbase_name = pecha.set_base(\"base content\", metadata={\"title\": \"title\", \"order\": 1})\npecha.set_layer(base_name, layer)\n\npecha.save()\n\nassert pecha.layers[base_name][LayerEnum.citation].id == layer.id\n</code></pre> <p>After successfully parsing the input, you should be able to get base text and layers.</p> <p>Since, layers are based on the base text, first you need to set base to <code>pecha</code> with <code>pecha.set_base('base content</code>) which will return <code>base_name</code>, which is identifier for base and it's associated layers. Then you use <code>base_name</code> to set layer to <code>pecha</code>.</p> <p>Notice, you can also set base metadata with <code>metadata</code> argument in <code>pecha.set_base()</code> method.</p> <p>After adding all the bases and layer, call <code>pecha.save()</code> to save bases and layers.</p>"},{"location":"toolkit/pecha/#update-base-layer","title":"Update Base layer","text":"<p>In order to update a base, we need to know the <code>base_name</code> of base that we want to update.</p> <pre><code>from openpecha.core.pecha import OpenPechaFS\n\npecha = OpenPechaFS(path=\"&lt;path_to_pecha&gt;\")\n\npecha.update_base(\"v001\", \"new content\")\npecha.save()\n</code></pre>"},{"location":"toolkit/publishing-pipeline/","title":"Publishing pipeline","text":""},{"location":"toolkit/toolkit-overview/","title":"Python library overview","text":""},{"location":"tools/cloud-vision-key/","title":"How to get a Google Cloud Vision key to use with the OCR Pipeline","text":"<p>The OCR Pipeline uses Google Cloud Vision, part of the Google Cloud platform to OCR images.</p> <p>To use the OCR Pipeline, you need a Google Cloud Vision key.</p> <p>To get one, you need:</p> <ul> <li>A Google account. Get one here.</li> <li>A credit card or debit card</li> </ul> <p>Note If you don't have a credit card, contact openpecha@gmail.com. Our partners at pecha.jobs might be able to OCR images for you.</p>"},{"location":"tools/cloud-vision-key/#on-this-page","title":"On this page:","text":"<ul> <li> Creating a Google Cloud account</li> <li> Getting a Cloud Vision key</li> <li> Help</li> </ul>"},{"location":"tools/cloud-vision-key/#how-to-create-a-google-cloud-account","title":"How to create a Google Cloud account","text":""},{"location":"tools/cloud-vision-key/#1-sign-into-google","title":"1. Sign into Google","text":"<p>Make sure you're signed into Google.</p>"},{"location":"tools/cloud-vision-key/#2-go-to-google-cloud-and-get-started","title":"2. Go to Google Cloud and get started","text":"<p>Visit https://cloud.google.com/ and select Console.\u00a0</p> <p></p> <p>Note The Google Cloud home page might not look exactly like this when you visit it. If you don't see Console, select Get Started or Get started for free. The sign-up process is very similar.</p>"},{"location":"tools/cloud-vision-key/#3-agree-to-the-terms-of-service-for-your-country","title":"3. Agree to the terms of service for your country.","text":"<p>Select your country and check the box under Terms of Service to agree to it.</p> <p>Then select agree and continue.</p> <p></p> <p>Note If you selected Get started or Get started for free on the Google Cloud home page, you might see a screen like this instead:</p> <p></p>"},{"location":"tools/cloud-vision-key/#4-activate-your-google-cloud-account","title":"4. Activate your Google Cloud account","text":""},{"location":"tools/cloud-vision-key/#41-select-activate-to-start-activating-your-account","title":"4.1 Select Activate to start activating your account","text":""},{"location":"tools/cloud-vision-key/#42-fill-in-your-account-details","title":"4.2 Fill in your account details","text":"<ol> <li>Select your country (if necessary).</li> <li>Select a description of your organization or needs.</li> <li>Check the box under Terms of Service to agree to it.</li> <li>Select Continue.</li> </ol>"},{"location":"tools/cloud-vision-key/#43-create-a-new-payments-profile","title":"4.3 Create a new payments profile","text":"<ol> <li>Select Create a new payments profile or Submit.</li> </ol> <ol> <li>Select Add payment method to open a window where you can add a credit card or debit card.</li> </ol> <p>Note Depending on your country, you might also have an option to add a bank account.</p> <ol> <li>Choose a payment type and add your address. Then select Create.</li> </ol> <p>For profile type:  - Select individual if you are using this account for yourself and using your personal payment info.  - Select organization if you are using this account at an organization and are using its payment method.</p> <p></p> <ol> <li>Add your account information and select Save card.</li> </ol> <p></p>"},{"location":"tools/cloud-vision-key/#44-select-start-my-free-trial","title":"4.4 Select Start my free trial","text":"<p>Your account is now complete. Google Cloud should look now like something like this:</p> <p></p>"},{"location":"tools/cloud-vision-key/#how-to-get-a-google-cloud-vision-key","title":"How to get a Google Cloud Vision key","text":"<p>Now that you have a Google Cloud account, it's time to get a Cloud Vision key.</p>"},{"location":"tools/cloud-vision-key/#1-open-my-first-project","title":"1. Open My first project","text":"<p>On the Google Cloud dashboard, find My first project and select Go to project details. Or go to the dropdown project menu at the top of the dashboard, find My first project, and open it.</p> <p></p>"},{"location":"tools/cloud-vision-key/#2-add-google-cloud-vision-to-your-project","title":"2. Add Google Cloud Vision to your project","text":"<ol> <li>With My first project open, enter Cloud Vision in Google Cloud console's search bar and choose Cloud Vision API. </li> </ol> <ol> <li>Select Enable to add the Cloud Vision API to your project.</li> </ol>"},{"location":"tools/cloud-vision-key/#3-create-cloud-vision-credentials","title":"3. Create Cloud Vision credentials","text":"<ol> <li>On the APIs &amp; Services screen, select Credentials &gt; Create credentials &gt; Service account.</li> </ol> <ol> <li>Add a service account name and select done.</li> </ol> <ol> <li>Select the service account email address to open details about it.</li> </ol> <ol> <li>Select Keys &gt; Add key &gt; Create new key.</li> </ol> <ol> <li>Then with JSON chosen, select Create to create a new key.</li> </ol> <p>Google will send you a <code>.json</code> file. This is your key for the ORC Pipeline.</p> <p>Congratulations! Now you are ready to use the OCR Pipeline.</p>"},{"location":"tools/ocr-pipeline-reference/","title":"OCR Pipeline reference","text":"<p>The OCR Pipeline provides an interface for OCRing scanned texts in the BDRC library. </p> <p>Given a BDRC Scan ID, the OCR Pipeline:</p> <ol> <li>Retrieves the scans that make up the text from the BDRC library.</li> <li>OCRs them with Google Cloud Vision.</li> <li>Converts the results into the OPF format using the OpenPecha toolkit.</li> <li>Creates a new repo on OpenPecha Data's GitHub.</li> <li>Puts the OPF files for the text into the new repo.</li> </ol>"},{"location":"tools/ocr-pipeline-reference/#on-this-page","title":"On this page","text":"<ul> <li> Input reference</li> <li> Output reference</li> <li> Processing time</li> <li> Help</li> </ul>"},{"location":"tools/ocr-pipeline-reference/#input-reference","title":"Input reference","text":"<p>Note Using the OCR Pipeline requires a Google Cloud Vision service account key. Learn how to get one here.</p>"},{"location":"tools/ocr-pipeline-reference/#email","title":"Email","text":"<p>Adding your email prompts your browser to save your email and key in your browser settings so you don't have to reenter them every time you use this tool.</p>"},{"location":"tools/ocr-pipeline-reference/#google-cloud-service-json-key-file","title":"Google Cloud Service JSON key file","text":"<p>This is contained in the <code>.json</code> file that Google Cloud provides as a key for its Cloud Vision service.</p> <p>Open the file in a text editor and copy the JSON code into this field.</p> <p>Note If you don't have a key or you need help getting one, read this guide.</p>"},{"location":"tools/ocr-pipeline-reference/#name","title":"Name","text":"<p>In this field, you can name the batch that you are scanning.</p>"},{"location":"tools/ocr-pipeline-reference/#input","title":"Input","text":"<p>The OCR Pipeline currently only supports OCRing images in the BDRC library. Texts are retrieved using their BDRC Scan ID.</p> <p>Example of a BDRC scan ID:</p> <p></p> <p>The scan ID follows <code>bdr:</code>. In this case, the ID is <code>W1KG12304</code>.</p> <p>Multiple scans can be OCRd in one batch. Add one BDRC Scan ID per line.</p> <p>Warning BDRC Work IDs and Version IDs aren't supported. If used, the OCR will result in failure.</p>"},{"location":"tools/ocr-pipeline-reference/#ocr-engine","title":"OCR Engine","text":"<p>The OCR Pipeline currently only supports Google Cloud Vision.</p>"},{"location":"tools/ocr-pipeline-reference/#model-type","title":"Model Type","text":"<p>These model types are accessible by the OCR Pipeline.</p> <ul> <li>builtin/stable</li> <li>builtin/latest</li> <li>builtin/weekly</li> </ul> <p><code>builtin/weekly</code> seems to produce the best results, but this needs more testing. Feel free to experiment.</p> <p>Warning <code>builtin/stable</code> doesn't currently work for Tibetan.</p>"},{"location":"tools/ocr-pipeline-reference/#language-hint","title":"Language Hint","text":"<p>The OP Pipeline can use these language hints to improve results:</p> <ul> <li>Auto</li> <li>Tibetan</li> <li>Tibet-handwriting</li> <li>Chinese</li> <li>Chinese-hanwriting</li> <li>Devanagari</li> <li>Davenagari-handwriting</li> </ul> <p><code>Auto</code> seems to produce the best results, but this needs more testing. Feel free to experiment.</p>"},{"location":"tools/ocr-pipeline-reference/#sponsor-name","title":"Sponsor Name","text":"<p>The could be your name, your organization's name, or the person who bought the Google Cloud credit used to OCR the text(s) in this job. The name that is entered gets added to the OPF metadata. </p>"},{"location":"tools/ocr-pipeline-reference/#allow-bdrc-and-openpecha-to-use-the-results-to-improve-this-service","title":"Allow BDRC and OpenPecha to use the results to improve this service","text":"<p>By ticking this box, the results get put in a public OpenPecha Data repository on GitHub and you agree to allow BDRC and OpenPecha to use the resulting data.</p> <p>If you don't agree, the file will be put in a private repo on OpenPecha Data's GitHub. In this case, after your job is successfully completed, email us at openpecha[at]gmail.com for access.</p>"},{"location":"tools/ocr-pipeline-reference/#output-reference","title":"Output reference","text":""},{"location":"tools/ocr-pipeline-reference/#all-batches","title":"All Batches","text":"<p>The right side of the OCR Pipeline interface contains a list of recent batches of files that have been processed. Select Details next to your batch to see its progress and results.</p>"},{"location":"tools/ocr-pipeline-reference/#batch-detail","title":"Batch Detail","text":"<p>Here you can:</p> <ul> <li>Select the link under Result to go to the repo(s) that contain(s) the OCRd file(s).</li> <li>Toggle the chevron next to Inputs to see the list of files that were OCRd.</li> <li>Toggle the chevron next to Pipeline Config to see the language hint, model type, and OCR engine that were used.</li> <li>Select Details under Actions to see more metadata about the batch.</li> </ul> <p></p>"},{"location":"tools/ocr-pipeline-reference/#error-messages","title":"Error messages","text":"<ul> <li><code>FileNotFoundError</code>: The supplied ID(s) weren't found. This could be because the supplied ID(s) were BDRC Work IDs or Version IDs.</li> <li> <p>Solution: Find the Scan ID for the text(s) you'd like to OCR and try again.</p> </li> <li> <p><code>AttributeError: 'str' object has no attribute 'keys'</code>: The provided key wasn't in the correct format. This could be because you entered the name of key file instead of the contents of the file.</p> </li> <li> <p>Solution: See the directions above for entering your Google Cloud Service JSON key file.  </p> </li> <li> <p><code>GoogleVisionCredentialsError</code>: The supplied key is correctly formatted, but may have expired.</p> </li> <li>Solution: Regenerate a key on Google Cloud Vision and try again.</li> </ul>"},{"location":"tools/ocr-pipeline-reference/#processing-time","title":"Processing time","text":"<p>Processing an OCR job may take several minutes or more, depending on the number of images that are scanned.</p>"},{"location":"tools/ocr-pipeline-reference/#need-help","title":"Need help?","text":"<ul> <li>File an issue.</li> <li>Join our Discord and ask there.</li> <li>Email us at openpecha[at]gmail.com.</li> </ul>"},{"location":"tools/overview/","title":"Tools overview","text":"<ul> <li> <p> OCR Pipeline</p> <p>Use the OCR Pipeline on scanned texts in the BDRC library and get the results in an OpenPecha Data repository on GitHub.</p> <p> OCR Pipeline</p> </li> </ul>"}]}